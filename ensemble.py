# ensemble.py

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
import logging
from abc import ABC, abstractmethod
import pickle
from pathlib import Path
import gc
import time

from sklearn.linear_model import LogisticRegression, RidgeCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit, KFold
from sklearn.metrics import roc_auc_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor

try:
    import optuna
    from optuna.samplers import TPESampler
    from optuna.pruners import MedianPruner, HyperbandPruner
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    logging.warning("Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

from config import Config
from models import BaseModel
from evaluation import CTRMetrics

logger = logging.getLogger(__name__)

class BaseEnsemble(ABC):
    """ì•™ìƒë¸” ëª¨ë¸ ê¸°ë³¸ í´ë˜ìŠ¤ - 1070ë§Œí–‰ ìµœì í™”"""
    
    def __init__(self, name: str):
        self.name = name
        self.base_models = {}
        self.is_fitted = False
        self.training_data_size = 0
        self.target_ctr = 0.0201
        
    @abstractmethod
    def fit(self, X: pd.DataFrame, y: pd.Series, base_predictions: Dict[str, np.ndarray]):
        """ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ"""
        pass
    
    @abstractmethod
    def predict_proba(self, base_predictions: Dict[str, np.ndarray]) -> np.ndarray:
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        pass
    
    def add_base_model(self, name: str, model: BaseModel):
        """ê¸°ë³¸ ëª¨ë¸ ì¶”ê°€"""
        self.base_models[name] = model
    
    def get_base_predictions(self, X: pd.DataFrame) -> Dict[str, np.ndarray]:
        """ëª¨ë“  ê¸°ë³¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘ - ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬"""
        predictions = {}
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬
        batch_size = 100000
        total_size = len(X)
        
        logger.info(f"ê¸°ë³¸ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘ ì‹œì‘ (ë°ì´í„° í¬ê¸°: {total_size:,}í–‰)")
        
        for name, model in self.base_models.items():
            try:
                model_predictions = []
                
                for i in range(0, total_size, batch_size):
                    end_idx = min(i + batch_size, total_size)
                    X_batch = X.iloc[i:end_idx]
                    
                    batch_pred = model.predict_proba(X_batch)
                    model_predictions.append(batch_pred)
                    
                    if (i // batch_size + 1) % 10 == 0:
                        logger.info(f"{name} ëª¨ë¸ ì˜ˆì¸¡ ì§„í–‰: {end_idx:,}/{total_size:,}")
                
                predictions[name] = np.concatenate(model_predictions)
                logger.info(f"{name} ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"{name} ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
                predictions[name] = np.full(total_size, self.target_ctr)
        
        return predictions

class CTROptimalEnsemble(BaseEnsemble):
    """CTR ì˜ˆì¸¡ ìµœì í™” ì•™ìƒë¸” - Combined Score 0.30+ ëª©í‘œ"""
    
    def __init__(self, target_ctr: float = 0.0201, optimization_method: str = 'combined_plus'):
        super().__init__("CTROptimalEnsemble")
        self.target_ctr = target_ctr
        self.optimization_method = optimization_method
        self.final_weights = {}
        self.ctr_calibrator = None
        self.metrics_calculator = CTRMetrics()
        self.temperature = 1.0
        self.bias_correction = 0.0
        self.diversity_bonus = {}
        self.performance_weights = {}
        
    def fit(self, X: pd.DataFrame, y: pd.Series, base_predictions: Dict[str, np.ndarray]):
        """CTR ìµœì í™” ì•™ìƒë¸” í•™ìŠµ - Combined Score 0.30+ ëª©í‘œ"""
        logger.info(f"CTR ìµœì í™” ì•™ìƒë¸” í•™ìŠµ ì‹œì‘ - ë°©ë²•: {self.optimization_method}")
        logger.info(f"ëª©í‘œ: Combined Score 0.30+")
        
        available_models = list(base_predictions.keys())
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")
        
        if len(available_models) < 2:
            logger.warning("ì•™ìƒë¸”ì„ ìœ„í•œ ëª¨ë¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            if available_models:
                self.final_weights = {available_models[0]: 1.0}
            self.is_fitted = True
            return
        
        self.training_data_size = len(X)
        
        # 1ë‹¨ê³„: ê³ ê¸‰ ë‹¤ë‹¨ê³„ ê°€ì¤‘ì¹˜ ìµœì í™”
        self.final_weights = self._advanced_multi_stage_optimization(base_predictions, y)
        
        # 2ë‹¨ê³„: CTR íŠ¹í™” í›„ì²˜ë¦¬ ìµœì í™”
        ensemble_pred = self._create_weighted_ensemble(base_predictions)
        self._optimize_ctr_postprocessing_advanced(ensemble_pred, y)
        
        # 3ë‹¨ê³„: ì„±ëŠ¥ ê²€ì¦
        final_score = self._validate_performance(base_predictions, y)
        
        self.is_fitted = True
        
        if final_score >= 0.30:
            logger.info(f"ğŸ¯ CTR ìµœì í™” ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ - Combined Score: {final_score:.4f} (ëª©í‘œ ë‹¬ì„±!)")
        else:
            logger.warning(f"âš ï¸ CTR ìµœì í™” ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ - Combined Score: {final_score:.4f} (ëª©í‘œ ë¯¸ë‹¬ì„±)")
    
    def _advanced_multi_stage_optimization(self, base_predictions: Dict[str, np.ndarray], y: pd.Series) -> Dict[str, float]:
        """ê³ ê¸‰ ë‹¤ë‹¨ê³„ ê°€ì¤‘ì¹˜ ìµœì í™” - Combined Score 0.30+ ëª©í‘œ"""
        
        model_names = list(base_predictions.keys())
        
        # 1ë‹¨ê³„: ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ê°•í™”)
        individual_scores = {}
        for name, pred in base_predictions.items():
            combined_score = self.metrics_calculator.combined_score(y, pred)
            ap_score = self.metrics_calculator.average_precision(y, pred)
            wll_score = self.metrics_calculator.weighted_log_loss(y, pred)
            
            # CTR í¸í–¥ ì ìˆ˜
            predicted_ctr = pred.mean()
            actual_ctr = y.mean()
            ctr_bias = abs(predicted_ctr - actual_ctr)
            ctr_score = np.exp(-ctr_bias * 200)  # ë” ê°•í•œ CTR í¸í–¥ íŒ¨ë„í‹°
            
            # ì˜ˆì¸¡ ë‹¤ì–‘ì„± ì ìˆ˜
            diversity_score = self._calculate_prediction_diversity(pred)
            
            # ì¢…í•© ì„±ëŠ¥ ì ìˆ˜
            total_score = (0.4 * combined_score + 0.2 * ap_score + 
                          0.2 * (1/(1+wll_score)) + 0.1 * ctr_score + 0.1 * diversity_score)
            
            individual_scores[name] = total_score
            self.performance_weights[name] = total_score
            
        logger.info(f"ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ (ê°•í™”): {individual_scores}")
        
        # 2ë‹¨ê³„: ë‹¤ì–‘ì„± ë¶„ì„ ë° ë³´ë„ˆìŠ¤
        diversity_matrix = self._calculate_diversity_matrix(base_predictions)
        self.diversity_bonus = self._calculate_diversity_bonus(diversity_matrix, individual_scores)
        
        # 3ë‹¨ê³„: ì´ˆê¸° ê°€ì¤‘ì¹˜ ì„¤ì • (ì„±ëŠ¥ + ë‹¤ì–‘ì„±)
        initial_weights = {}
        for name in model_names:
            base_weight = individual_scores[name]
            diversity_weight = self.diversity_bonus.get(name, 0.0)
            initial_weights[name] = base_weight + diversity_weight
        
        # ì •ê·œí™”
        total_weight = sum(initial_weights.values())
        if total_weight > 0:
            initial_weights = {k: v/total_weight for k, v in initial_weights.items()}
        else:
            initial_weights = {name: 1.0/len(model_names) for name in model_names}
        
        # 4ë‹¨ê³„: ê³„ì¸µì  ê·¸ë¦¬ë“œ ì„œì¹˜
        optimized_weights = self._hierarchical_grid_search(base_predictions, y, initial_weights)
        
        # 5ë‹¨ê³„: ê³ ê¸‰ Optuna ìµœì í™”
        if OPTUNA_AVAILABLE and len(model_names) <= 5:
            try:
                final_weights = self._advanced_optuna_optimization(base_predictions, y, optimized_weights)
            except Exception as e:
                logger.warning(f"ê³ ê¸‰ Optuna ìµœì í™” ì‹¤íŒ¨: {e}")
                final_weights = optimized_weights
        else:
            final_weights = optimized_weights
        
        # 6ë‹¨ê³„: ìµœì¢… ì„±ëŠ¥ ê²€ì¦ ë° ì¡°ì •
        final_weights = self._performance_based_adjustment(base_predictions, y, final_weights)
        
        logger.info(f"ìµœì¢… ê°€ì¤‘ì¹˜: {final_weights}")
        return final_weights
    
    def _calculate_prediction_diversity(self, predictions: np.ndarray) -> float:
        """ì˜ˆì¸¡ ë‹¤ì–‘ì„± ê³„ì‚°"""
        try:
            unique_count = len(np.unique(predictions))
            expected_diversity = max(1000, len(predictions) // 10000)  # 1070ë§Œí–‰ ê¸°ì¤€
            
            diversity_ratio = min(unique_count / expected_diversity, 1.0)
            
            # ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ë‹¤ì–‘ì„±
            p = np.clip(predictions, 1e-15, 1 - 1e-15)
            entropy = -np.mean(p * np.log2(p) + (1 - p) * np.log2(1 - p))
            entropy_score = entropy / np.log2(2)  # ì •ê·œí™”
            
            # ë¶„ì‚° ê¸°ë°˜ ë‹¤ì–‘ì„±
            variance_score = min(np.std(predictions) / 0.1, 1.0)
            
            diversity_score = 0.4 * diversity_ratio + 0.3 * entropy_score + 0.3 * variance_score
            
            return diversity_score
        except Exception as e:
            logger.warning(f"ì˜ˆì¸¡ ë‹¤ì–‘ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_diversity_matrix(self, base_predictions: Dict[str, np.ndarray]) -> Dict[str, Dict[str, float]]:
        """ë‹¤ì–‘ì„± í–‰ë ¬ ê³„ì‚° - 1070ë§Œí–‰ ìµœì í™”"""
        model_names = list(base_predictions.keys())
        diversity_matrix = {}
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§
        sample_size = min(100000, len(list(base_predictions.values())[0]))
        if sample_size < len(list(base_predictions.values())[0]):
            sample_indices = np.random.choice(len(list(base_predictions.values())[0]), sample_size, replace=False)
            sampled_predictions = {name: pred[sample_indices] for name, pred in base_predictions.items()}
        else:
            sampled_predictions = base_predictions
        
        for name1 in model_names:
            diversity_matrix[name1] = {}
            for name2 in model_names:
                if name1 == name2:
                    diversity_matrix[name1][name2] = 0.0
                else:
                    try:
                        # Pearson ìƒê´€ê³„ìˆ˜
                        corr = np.corrcoef(sampled_predictions[name1], sampled_predictions[name2])[0, 1]
                        if np.isnan(corr):
                            corr = 0.0
                        
                        # ìˆœìœ„ ìƒê´€ê³„ìˆ˜
                        rank_corr = self._calculate_rank_correlation(
                            sampled_predictions[name1], sampled_predictions[name2]
                        )
                        
                        # KL Divergence
                        kl_div = self._calculate_kl_divergence(
                            sampled_predictions[name1], sampled_predictions[name2]
                        )
                        
                        # ì¢…í•© ë‹¤ì–‘ì„± ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨)
                        diversity_score = 0.4 * abs(corr) + 0.3 * abs(rank_corr) + 0.3 * (1 - kl_div)
                        diversity_matrix[name1][name2] = diversity_score
                        
                    except Exception as e:
                        logger.warning(f"{name1}-{name2} ë‹¤ì–‘ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
                        diversity_matrix[name1][name2] = 0.5
        
        return diversity_matrix
    
    def _calculate_rank_correlation(self, pred1: np.ndarray, pred2: np.ndarray) -> float:
        """ìˆœìœ„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        try:
            from scipy.stats import spearmanr
            corr, _ = spearmanr(pred1, pred2)
            return corr if not np.isnan(corr) else 0.0
        except:
            # ê°„ë‹¨í•œ ìˆœìœ„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            rank1 = pd.Series(pred1).rank(pct=True)
            rank2 = pd.Series(pred2).rank(pct=True)
            return np.corrcoef(rank1, rank2)[0, 1] if not np.isnan(np.corrcoef(rank1, rank2)[0, 1]) else 0.0
    
    def _calculate_kl_divergence(self, pred1: np.ndarray, pred2: np.ndarray) -> float:
        """KL Divergence ê³„ì‚°"""
        try:
            # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ KL Divergence
            bins = 50
            hist1, _ = np.histogram(pred1, bins=bins, range=(0, 1), density=True)
            hist2, _ = np.histogram(pred2, bins=bins, range=(0, 1), density=True)
            
            # í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
            p = hist1 / (hist1.sum() + 1e-15)
            q = hist2 / (hist2.sum() + 1e-15)
            
            # KL Divergence
            kl = np.sum(p * np.log((p + 1e-15) / (q + 1e-15)))
            
            # ì •ê·œí™” (0-1 ë²”ìœ„)
            return min(1.0, kl / 10.0)
        except Exception as e:
            logger.warning(f"KL Divergence ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    def _calculate_diversity_bonus(self, diversity_matrix: Dict[str, Dict[str, float]], 
                                 performance_scores: Dict[str, float]) -> Dict[str, float]:
        """ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤ ê³„ì‚°"""
        diversity_bonus = {}
        
        for model_name in diversity_matrix.keys():
            # ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ì˜ í‰ê·  ë‹¤ì–‘ì„± (ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘í•¨)
            other_models = [m for m in diversity_matrix.keys() if m != model_name]
            if other_models:
                avg_diversity = np.mean([diversity_matrix[model_name][other] for other in other_models])
                
                # ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤ (ë‹¤ì–‘í• ìˆ˜ë¡ ë†’ì€ ë³´ë„ˆìŠ¤)
                diversity_bonus_value = (1.0 - avg_diversity) * 0.1
                
                # ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì—ê²Œ ë” í° ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
                performance_multiplier = performance_scores.get(model_name, 0.5)
                final_bonus = diversity_bonus_value * performance_multiplier
                
                diversity_bonus[model_name] = final_bonus
            else:
                diversity_bonus[model_name] = 0.0
        
        logger.info(f"ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤: {diversity_bonus}")
        return diversity_bonus
    
    def _hierarchical_grid_search(self, base_predictions: Dict[str, np.ndarray], y: pd.Series,
                                initial_weights: Dict[str, float]) -> Dict[str, float]:
        """ê³„ì¸µì  ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœì í™”"""
        model_names = list(base_predictions.keys())
        best_weights = initial_weights.copy()
        best_score = self._evaluate_ensemble(base_predictions, y, best_weights)
        
        logger.info(f"ê³„ì¸µì  ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹œì‘ - ì´ˆê¸° ì ìˆ˜: {best_score:.4f}")
        
        # 3ë‹¨ê³„ ê³„ì¸µì  ìµœì í™”
        adjustment_steps = [0.1, 0.03, 0.01]
        
        for step_level, step in enumerate(adjustment_steps):
            logger.info(f"ê³„ì¸µ {step_level + 1} ìµœì í™” (step: {step})")
            improved = True
            iteration = 0
            
            while improved and iteration < 30:
                improved = False
                iteration += 1
                
                for target_model in model_names:
                    for direction in [-1, 1]:
                        # ê°€ì¤‘ì¹˜ ì¡°ì • ì‹œë„
                        test_weights = best_weights.copy()
                        test_weights[target_model] += direction * step
                        
                        # ìŒìˆ˜ ë°©ì§€
                        if test_weights[target_model] < 0.01:
                            continue
                        
                        # ì •ê·œí™”
                        total_weight = sum(test_weights.values())
                        if total_weight > 0:
                            test_weights = {k: v/total_weight for k, v in test_weights.items()}
                        else:
                            continue
                        
                        # í‰ê°€
                        score = self._evaluate_ensemble(base_predictions, y, test_weights)
                        
                        if score > best_score + 1e-6:  # ë¯¸ì„¸í•œ ê°œì„ ë„ ìˆ˜ìš©
                            best_score = score
                            best_weights = test_weights
                            improved = True
                            
                            if score >= 0.30:
                                logger.info(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±! ì ìˆ˜: {score:.4f}")
        
        logger.info(f"ê³„ì¸µì  ê·¸ë¦¬ë“œ ì„œì¹˜ ì™„ë£Œ - ìµœì¢… ì ìˆ˜: {best_score:.4f}")
        return best_weights
    
    def _advanced_optuna_optimization(self, base_predictions: Dict[str, np.ndarray], y: pd.Series,
                                    initial_weights: Dict[str, float]) -> Dict[str, float]:
        """ê³ ê¸‰ Optuna ê¸°ë°˜ ì •ë°€ ê°€ì¤‘ì¹˜ ìµœì í™”"""
        
        model_names = list(base_predictions.keys())
        
        def advanced_objective(trial):
            weights = {}
            
            # ì´ˆê¸°ê°’ ê¸°ë°˜ ë²”ìœ„ ì„¤ì • (ë” ë„“ì€ ë²”ìœ„)
            for name in model_names:
                initial_val = initial_weights.get(name, 1.0/len(model_names))
                low_bound = max(0.01, initial_val - 0.4)
                high_bound = min(0.99, initial_val + 0.4)
                weights[name] = trial.suggest_float(f'weight_{name}', low_bound, high_bound)
            
            # ì •ê·œí™”
            total_weight = sum(weights.values())
            if total_weight > 0:
                weights = {k: v/total_weight for k, v in weights.items()}
            else:
                return 0.0
            
            score = self._evaluate_ensemble(base_predictions, y, weights)
            
            # Combined Score 0.30+ ëª©í‘œì— ëŒ€í•œ ë³´ë„ˆìŠ¤
            if score >= 0.30:
                bonus = (score - 0.30) * 10  # ëª©í‘œ ì´ˆê³¼ì‹œ í° ë³´ë„ˆìŠ¤
                return score + bonus
            
            return score
        
        study = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(
                seed=42,
                n_startup_trials=20,
                n_ei_candidates=48,
                multivariate=True
            ),
            pruner=HyperbandPruner(
                min_resource=5,
                max_resource=50,
                reduction_factor=4
            )
        )
        
        study.optimize(advanced_objective, n_trials=200, show_progress_bar=False)
        
        optimized_weights = {}
        for param_name, weight in study.best_params.items():
            model_name = param_name.replace('weight_', '')
            optimized_weights[model_name] = weight
        
        # ì •ê·œí™”
        total_weight = sum(optimized_weights.values())
        if total_weight > 0:
            optimized_weights = {k: v/total_weight for k, v in optimized_weights.items()}
        
        logger.info(f"ê³ ê¸‰ Optuna ìµœì í™” ì™„ë£Œ - ì ìˆ˜: {study.best_value:.4f}")
        return optimized_weights
    
    def _performance_based_adjustment(self, base_predictions: Dict[str, np.ndarray], y: pd.Series,
                                    weights: Dict[str, float]) -> Dict[str, float]:
        """ì„±ëŠ¥ ê¸°ë°˜ ìµœì¢… ì¡°ì •"""
        
        # í˜„ì¬ ì•™ìƒë¸” ì„±ëŠ¥
        current_score = self._evaluate_ensemble(base_predictions, y, weights)
        
        if current_score >= 0.30:
            logger.info(f"ëª©í‘œ ë‹¬ì„±ìœ¼ë¡œ ì¡°ì • ìƒëµ: {current_score:.4f}")
            return weights
        
        # ì„±ëŠ¥ì´ ë‚®ì€ ê²½ìš° top performerì— ë” ë§ì€ ê°€ì¤‘ì¹˜
        adjusted_weights = weights.copy()
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_model = max(self.performance_weights.keys(), 
                        key=lambda x: self.performance_weights[x])
        
        # ê°€ì¤‘ì¹˜ ì¬ë¶„ë°° (ë³´ìˆ˜ì )
        redistribution_factor = 0.1
        weight_to_redistribute = 0.0
        
        for model_name in adjusted_weights.keys():
            if model_name != best_model:
                reduction = adjusted_weights[model_name] * redistribution_factor
                adjusted_weights[model_name] -= reduction
                weight_to_redistribute += reduction
        
        adjusted_weights[best_model] += weight_to_redistribute
        
        # ì •ê·œí™”
        total_weight = sum(adjusted_weights.values())
        if total_weight > 0:
            adjusted_weights = {k: v/total_weight for k, v in adjusted_weights.items()}
        
        # ì„±ëŠ¥ ê²€ì¦
        new_score = self._evaluate_ensemble(base_predictions, y, adjusted_weights)
        
        if new_score > current_score:
            logger.info(f"ì„±ëŠ¥ ê¸°ë°˜ ì¡°ì • ì„±ê³µ: {current_score:.4f} â†’ {new_score:.4f}")
            return adjusted_weights
        else:
            logger.info(f"ì„±ëŠ¥ ê¸°ë°˜ ì¡°ì • ì·¨ì†Œ: {new_score:.4f} < {current_score:.4f}")
            return weights
    
    def _evaluate_ensemble(self, base_predictions: Dict[str, np.ndarray], y: pd.Series, 
                         weights: Dict[str, float]) -> float:
        """ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ - Combined Score 0.30+ ëª©í‘œ"""
        try:
            ensemble_pred = np.zeros(len(y))
            for name, weight in weights.items():
                if name in base_predictions:
                    ensemble_pred += weight * base_predictions[name]
            
            # Combined Score ê³„ì‚°
            combined_score = self.metrics_calculator.combined_score(y, ensemble_pred)
            
            # CTR í¸í–¥ íŒ¨ë„í‹° ê°•í™”
            predicted_ctr = ensemble_pred.mean()
            actual_ctr = y.mean()
            ctr_bias = abs(predicted_ctr - actual_ctr)
            ctr_penalty = np.exp(-ctr_bias * 300)  # ë§¤ìš° ê°•í•œ CTR í¸í–¥ íŒ¨ë„í‹°
            
            # ìµœì¢… ì ìˆ˜ (Combined Score 0.30+ ëª©í‘œ)
            final_score = combined_score * ctr_penalty
            
            return final_score
        except Exception as e:
            logger.warning(f"ì•™ìƒë¸” í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _optimize_ctr_postprocessing_advanced(self, predictions: np.ndarray, y: pd.Series):
        """CTR íŠ¹í™” ê³ ê¸‰ í›„ì²˜ë¦¬ ìµœì í™”"""
        logger.info("CTR ê³ ê¸‰ í›„ì²˜ë¦¬ ìµœì í™” ì‹œì‘")
        
        try:
            # 1. í¸í–¥ ë³´ì •
            predicted_ctr = predictions.mean()
            actual_ctr = y.mean()
            self.bias_correction = actual_ctr - predicted_ctr
            
            # 2. ê³ ê¸‰ Temperature scaling ìµœì í™”
            self._optimize_advanced_temperature_scaling(predictions, y)
            
            # 3. CTR ë¶„í¬ ë§¤ì¹­ ê³ ë„í™”
            self._optimize_advanced_distribution_matching(predictions, y)
            
            # 4. ë¶„ìœ„ìˆ˜ë³„ ë³´ì •
            self._optimize_quantile_correction(predictions, y)
            
            logger.info(f"CTR ê³ ê¸‰ í›„ì²˜ë¦¬ ìµœì í™” ì™„ë£Œ")
            logger.info(f"í¸í–¥: {self.bias_correction:.4f}, Temperature: {self.temperature:.3f}")
            
        except Exception as e:
            logger.error(f"CTR ê³ ê¸‰ í›„ì²˜ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
            self.bias_correction = 0.0
            self.temperature = 1.0
    
    def _optimize_advanced_temperature_scaling(self, predictions: np.ndarray, y: pd.Series):
        """ê³ ê¸‰ Temperature scaling ìµœì í™”"""
        try:
            from scipy.optimize import minimize_scalar
            
            def advanced_temperature_loss(temp):
                if temp <= 0.1 or temp > 10:
                    return float('inf')
                
                # Logit ë³€í™˜
                pred_clipped = np.clip(predictions, 1e-15, 1 - 1e-15)
                logits = np.log(pred_clipped / (1 - pred_clipped))
                
                # Temperature ì ìš©
                calibrated_logits = logits / temp
                calibrated_probs = 1 / (1 + np.exp(-calibrated_logits))
                calibrated_probs = np.clip(calibrated_probs, 1e-15, 1 - 1e-15)
                
                # Combined Score ê³„ì‚°
                combined_score = self.metrics_calculator.combined_score(y, calibrated_probs)
                
                # Combined Score 0.30+ ëª©í‘œì— ëŒ€í•œ ë³´ë„ˆìŠ¤/íŒ¨ë„í‹°
                if combined_score >= 0.30:
                    target_bonus = (combined_score - 0.30) * 100
                    return -(combined_score + target_bonus)  # ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜
                else:
                    target_penalty = (0.30 - combined_score) * 50
                    return -(combined_score - target_penalty)
            
            result = minimize_scalar(advanced_temperature_loss, bounds=(0.1, 10.0), method='bounded')
            self.temperature = result.x
            
        except Exception as e:
            logger.warning(f"ê³ ê¸‰ Temperature scaling ìµœì í™” ì‹¤íŒ¨: {e}")
            self.temperature = 1.0
    
    def _optimize_advanced_distribution_matching(self, predictions: np.ndarray, y: pd.Series):
        """ê³ ê¸‰ ë¶„í¬ ë§¤ì¹­ ìµœì í™”"""
        try:
            # CTR ë¶„í¬ íŠ¹ì„± ê³ ê¸‰ ë¶„ì„
            predicted_ctr = predictions.mean()
            actual_ctr = y.mean()
            
            # ë‹¤ì¤‘ ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë§¤ì¹­ (ë” ì„¸ë°€í•œ ë¶„ì„)
            pred_percentiles = np.percentile(predictions, [10, 25, 50, 75, 85, 90, 95, 97, 99])
            
            # ê³ CTR êµ¬ê°„ë³„ ì„¸ë°€í•œ ë³´ì •
            self.quantile_corrections = {}
            
            quantiles = [0.85, 0.90, 0.95, 0.97, 0.99]
            for i, q in enumerate(quantiles):
                threshold = np.percentile(predictions, q * 100)
                high_ctr_mask = predictions >= threshold
                
                if high_ctr_mask.sum() > 100:  # ì¶©ë¶„í•œ ìƒ˜í”Œì´ ìˆì„ ë•Œë§Œ
                    high_ctr_actual_rate = y[high_ctr_mask].mean()
                    high_ctr_pred_rate = predictions[high_ctr_mask].mean()
                    
                    if high_ctr_pred_rate > 0:
                        correction_factor = high_ctr_actual_rate / high_ctr_pred_rate
                        self.quantile_corrections[q] = {
                            'threshold': threshold,
                            'correction_factor': correction_factor
                        }
            
            logger.info(f"ê³ ê¸‰ ë¶„í¬ ë§¤ì¹­ ì™„ë£Œ - ë¶„ìœ„ìˆ˜ë³„ ë³´ì •: {len(self.quantile_corrections)}ê°œ")
            
        except Exception as e:
            logger.warning(f"ê³ ê¸‰ ë¶„í¬ ë§¤ì¹­ ìµœì í™” ì‹¤íŒ¨: {e}")
            self.quantile_corrections = {}
    
    def _optimize_quantile_correction(self, predictions: np.ndarray, y: pd.Series):
        """ë¶„ìœ„ìˆ˜ë³„ ë³´ì • ìµœì í™”"""
        try:
            # 10ë¶„ìœ„ìˆ˜ë³„ ì„¸ë°€í•œ ë³´ì •
            self.decile_corrections = {}
            
            for decile in range(1, 11):
                lower_bound = np.percentile(predictions, (decile - 1) * 10)
                upper_bound = np.percentile(predictions, decile * 10)
                
                decile_mask = (predictions >= lower_bound) & (predictions < upper_bound)
                
                if decile_mask.sum() > 100:
                    decile_actual_rate = y[decile_mask].mean()
                    decile_pred_rate = predictions[decile_mask].mean()
                    
                    if decile_pred_rate > 0:
                        correction_factor = decile_actual_rate / decile_pred_rate
                        self.decile_corrections[decile] = {
                            'lower_bound': lower_bound,
                            'upper_bound': upper_bound,
                            'correction_factor': correction_factor
                        }
            
            logger.info(f"ë¶„ìœ„ìˆ˜ë³„ ë³´ì • ì™„ë£Œ - 10ë¶„ìœ„ìˆ˜ ë³´ì •: {len(self.decile_corrections)}ê°œ")
            
        except Exception as e:
            logger.warning(f"ë¶„ìœ„ìˆ˜ë³„ ë³´ì • ì‹¤íŒ¨: {e}")
            self.decile_corrections = {}
    
    def _validate_performance(self, base_predictions: Dict[str, np.ndarray], y: pd.Series) -> float:
        """ìµœì¢… ì„±ëŠ¥ ê²€ì¦"""
        try:
            ensemble_pred = self._create_weighted_ensemble(base_predictions)
            calibrated_pred = self._apply_postprocessing_advanced(ensemble_pred)
            
            final_score = self.metrics_calculator.combined_score(y, calibrated_pred)
            
            # ìƒì„¸ ì„±ëŠ¥ ë¶„ì„
            ap_score = self.metrics_calculator.average_precision(y, calibrated_pred)
            wll_score = self.metrics_calculator.weighted_log_loss(y, calibrated_pred)
            
            predicted_ctr = calibrated_pred.mean()
            actual_ctr = y.mean()
            ctr_bias = abs(predicted_ctr - actual_ctr)
            
            logger.info(f"ìµœì¢… ì„±ëŠ¥ ê²€ì¦:")
            logger.info(f"  Combined Score: {final_score:.4f}")
            logger.info(f"  AP Score: {ap_score:.4f}")
            logger.info(f"  WLL Score: {wll_score:.4f}")
            logger.info(f"  CTR í¸í–¥: {ctr_bias:.4f}")
            logger.info(f"  ëª©í‘œ ë‹¬ì„±: {'âœ“' if final_score >= 0.30 else 'âœ—'}")
            
            return final_score
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _create_weighted_ensemble(self, base_predictions: Dict[str, np.ndarray]) -> np.ndarray:
        """ê°€ì¤‘ ì•™ìƒë¸” ìƒì„±"""
        ensemble_pred = np.zeros(len(list(base_predictions.values())[0]))
        
        for name, weight in self.final_weights.items():
            if name in base_predictions:
                ensemble_pred += weight * base_predictions[name]
        
        return ensemble_pred
    
    def _apply_postprocessing_advanced(self, predictions: np.ndarray) -> np.ndarray:
        """ê³ ê¸‰ í›„ì²˜ë¦¬ ì ìš©"""
        try:
            processed = predictions.copy()
            
            # 1. Temperature scaling
            if self.temperature != 1.0:
                pred_clipped = np.clip(processed, 1e-15, 1 - 1e-15)
                logits = np.log(pred_clipped / (1 - pred_clipped))
                calibrated_logits = logits / self.temperature
                processed = 1 / (1 + np.exp(-calibrated_logits))
            
            # 2. ë¶„ìœ„ìˆ˜ë³„ ë³´ì •
            if hasattr(self, 'quantile_corrections'):
                for quantile in sorted(self.quantile_corrections.keys(), reverse=True):
                    correction = self.quantile_corrections[quantile]
                    threshold = correction['threshold']
                    factor = correction['correction_factor']
                    
                    high_mask = processed >= threshold
                    if high_mask.sum() > 0:
                        processed[high_mask] *= factor
            
            # 3. 10ë¶„ìœ„ìˆ˜ë³„ ë³´ì •
            if hasattr(self, 'decile_corrections'):
                for decile in self.decile_corrections.keys():
                    correction = self.decile_corrections[decile]
                    lower_bound = correction['lower_bound']
                    upper_bound = correction['upper_bound']
                    factor = correction['correction_factor']
                    
                    decile_mask = (processed >= lower_bound) & (processed < upper_bound)
                    if decile_mask.sum() > 0:
                        processed[decile_mask] *= factor
            
            # 4. í¸í–¥ ë³´ì •
            processed = processed + self.bias_correction
            
            # 5. ìµœì¢… í´ë¦¬í•‘
            processed = np.clip(processed, 0.001, 0.999)
            
            return processed
            
        except Exception as e:
            logger.warning(f"ê³ ê¸‰ í›„ì²˜ë¦¬ ì ìš© ì‹¤íŒ¨: {e}")
            return np.clip(predictions, 0.001, 0.999)
    
    def predict_proba(self, base_predictions: Dict[str, np.ndarray]) -> np.ndarray:
        """ìµœì í™”ëœ ì•™ìƒë¸” ì˜ˆì¸¡ - Combined Score 0.30+ ëª©í‘œ"""
        if not self.is_fitted:
            raise ValueError("ì•™ìƒë¸” ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ê°€ì¤‘ ì•™ìƒë¸”
        ensemble_pred = self._create_weighted_ensemble(base_predictions)
        
        # ê³ ê¸‰ í›„ì²˜ë¦¬ ì ìš©
        calibrated_pred = self._apply_postprocessing_advanced(ensemble_pred)
        
        return calibrated_pred

class CTRStabilizedEnsemble(BaseEnsemble):
    """CTR ì˜ˆì¸¡ ì•ˆì •í™” ì•™ìƒë¸” - 1070ë§Œí–‰ ìµœì í™”"""
    
    def __init__(self, diversification_method: str = 'rank_weighted_advanced'):
        super().__init__("CTRStabilizedEnsemble")
        self.diversification_method = diversification_method
        self.model_weights = {}
        self.diversity_weights = {}
        self.final_weights = {}
        self.metrics_calculator = CTRMetrics()
        self.stability_factors = {}
        
    def fit(self, X: pd.DataFrame, y: pd.Series, base_predictions: Dict[str, np.ndarray]):
        """ì•ˆì •í™” ì•™ìƒë¸” í•™ìŠµ - 1070ë§Œí–‰ ìµœì í™”"""
        logger.info(f"CTR ì•ˆì •í™” ì•™ìƒë¸” í•™ìŠµ ì‹œì‘ - ë°©ë²•: {self.diversification_method}")
        
        available_models = list(base_predictions.keys())
        
        if len(available_models) < 2:
            logger.warning("ì•™ìƒë¸”ì„ ìœ„í•œ ëª¨ë¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            if available_models:
                self.final_weights = {available_models[0]: 1.0}
            self.is_fitted = True
            return
        
        self.training_data_size = len(X)
        
        # 1. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ì•ˆì •ì„± ê³ ë ¤)
        self.model_weights = self._evaluate_stability_performance(base_predictions, y)
        
        # 2. ê³ ê¸‰ ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜ ê³„ì‚°
        self.diversity_weights = self._calculate_advanced_diversity_weights(base_predictions)
        
        # 3. ì•ˆì •ì„± ìš”ì†Œ ê³„ì‚°
        self.stability_factors = self._calculate_stability_factors(base_predictions, y)
        
        # 4. ìµœì¢… ê°€ì¤‘ì¹˜ ê²°í•© (ì„±ëŠ¥ + ë‹¤ì–‘ì„± + ì•ˆì •ì„±)
        self.final_weights = self._combine_weights_with_stability()
        
        self.is_fitted = True
        logger.info(f"CTR ì•ˆì •í™” ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ - ìµœì¢… ê°€ì¤‘ì¹˜: {self.final_weights}")
    
    def _evaluate_stability_performance(self, base_predictions: Dict[str, np.ndarray], 
                                      y: pd.Series) -> Dict[str, float]:
        """ì•ˆì •ì„±ì„ ê³ ë ¤í•œ ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        
        performance_weights = {}
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ (íš¨ìœ¨ì )
        n_bootstrap = 10  # 1070ë§Œí–‰ì—ì„œëŠ” ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ ìˆ˜ ì œí•œ
        sample_size = min(200000, len(y))
        
        for name, pred in base_predictions.items():
            try:
                bootstrap_scores = []
                
                for i in range(n_bootstrap):
                    # ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ ìƒ˜í”Œ
                    sample_indices = np.random.choice(len(y), sample_size, replace=True)
                    y_sample = y.iloc[sample_indices]
                    pred_sample = pred[sample_indices]
                    
                    # Combined Score ê³„ì‚°
                    combined_score = self.metrics_calculator.combined_score(y_sample, pred_sample)
                    bootstrap_scores.append(combined_score)
                
                # ì•ˆì •ì„± ì§€í‘œ
                mean_score = np.mean(bootstrap_scores)
                std_score = np.std(bootstrap_scores)
                stability_score = mean_score / (1 + std_score)  # ì•ˆì •ì„± ë³´ì •
                
                performance_weights[name] = max(stability_score, 0.01)
                
                logger.info(f"{name} - í‰ê· : {mean_score:.4f}, í‘œì¤€í¸ì°¨: {std_score:.4f}, ì•ˆì •ì„±: {stability_score:.4f}")
                
            except Exception as e:
                logger.warning(f"{name} ì•ˆì •ì„± ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {e}")
                performance_weights[name] = 0.01
        
        return performance_weights
    
    def _calculate_advanced_diversity_weights(self, base_predictions: Dict[str, np.ndarray]) -> Dict[str, float]:
        """ê³ ê¸‰ ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜ ê³„ì‚° - 1070ë§Œí–‰ ìµœì í™”"""
        
        model_names = list(base_predictions.keys())
        diversity_weights = {}
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§
        sample_size = min(50000, len(list(base_predictions.values())[0]))
        sample_indices = np.random.choice(len(list(base_predictions.values())[0]), sample_size, replace=False)
        sampled_predictions = {name: pred[sample_indices] for name, pred in base_predictions.items()}
        
        if self.diversification_method == 'rank_weighted_advanced':
            # ê³ ê¸‰ ìˆœìœ„ ê¸°ë°˜ ë‹¤ì–‘ì„±
            for name in model_names:
                rank_differences = []
                prediction_distances = []
                distribution_differences = []
                
                for other in model_names:
                    if other != name:
                        # ìˆœìœ„ ì°¨ì´
                        rank_self = pd.Series(sampled_predictions[name]).rank(pct=True).values
                        rank_other = pd.Series(sampled_predictions[other]).rank(pct=True).values
                        rank_diff = np.mean(np.abs(rank_self - rank_other))
                        rank_differences.append(rank_diff)
                        
                        # ì˜ˆì¸¡ê°’ ê±°ë¦¬
                        pred_distance = np.mean(np.abs(sampled_predictions[name] - sampled_predictions[other]))
                        prediction_distances.append(pred_distance)
                        
                        # ë¶„í¬ ì°¨ì´ (íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜)
                        hist_self, _ = np.histogram(sampled_predictions[name], bins=20, range=(0, 1), density=True)
                        hist_other, _ = np.histogram(sampled_predictions[other], bins=20, range=(0, 1), density=True)
                        hist_diff = np.mean(np.abs(hist_self - hist_other))
                        distribution_differences.append(hist_diff)
                
                # ì¢…í•© ë‹¤ì–‘ì„± ì ìˆ˜
                rank_diversity = np.mean(rank_differences) if rank_differences else 0.5
                pred_diversity = np.mean(prediction_distances) if prediction_distances else 0.5
                dist_diversity = np.mean(distribution_differences) if distribution_differences else 0.5
                
                diversity_score = 0.4 * rank_diversity + 0.3 * pred_diversity + 0.3 * dist_diversity
                diversity_weights[name] = max(diversity_score, 0.1)
        
        elif self.diversification_method == 'correlation_matrix':
            # ê³ ê¸‰ ìƒê´€ê´€ê³„ í–‰ë ¬ ë¶„ì„
            correlation_matrix = self._calculate_correlation_matrix_advanced(sampled_predictions)
            
            for name in model_names:
                correlations = [abs(correlation_matrix[name][other]) 
                              for other in model_names if other != name]
                avg_correlation = np.mean(correlations) if correlations else 0.5
                diversity_score = 1.0 - avg_correlation
                diversity_weights[name] = max(diversity_score, 0.1)
        
        else:
            # ê¸°ë³¸ ë‹¤ì–‘ì„± (ê· ë“±)
            diversity_weights = {name: 1.0 for name in model_names}
        
        logger.info(f"ê³ ê¸‰ ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜: {diversity_weights}")
        return diversity_weights
    
    def _calculate_correlation_matrix_advanced(self, predictions: Dict[str, np.ndarray]) -> Dict[str, Dict[str, float]]:
        """ê³ ê¸‰ ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°"""
        model_names = list(predictions.keys())
        correlation_matrix = {}
        
        for name1 in model_names:
            correlation_matrix[name1] = {}
            for name2 in model_names:
                if name1 == name2:
                    correlation_matrix[name1][name2] = 1.0
                else:
                    try:
                        # Pearson ìƒê´€ê³„ìˆ˜
                        pearson_corr = np.corrcoef(predictions[name1], predictions[name2])[0, 1]
                        if np.isnan(pearson_corr):
                            pearson_corr = 0.0
                        
                        # Spearman ìƒê´€ê³„ìˆ˜ (ìˆœìœ„ ê¸°ë°˜)
                        rank1 = pd.Series(predictions[name1]).rank()
                        rank2 = pd.Series(predictions[name2]).rank()
                        spearman_corr = np.corrcoef(rank1, rank2)[0, 1]
                        if np.isnan(spearman_corr):
                            spearman_corr = 0.0
                        
                        # ì¢…í•© ìƒê´€ê³„ìˆ˜
                        combined_corr = 0.6 * abs(pearson_corr) + 0.4 * abs(spearman_corr)
                        correlation_matrix[name1][name2] = combined_corr
                        
                    except Exception as e:
                        logger.warning(f"{name1}-{name2} ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                        correlation_matrix[name1][name2] = 0.5
        
        return correlation_matrix
    
    def _calculate_stability_factors(self, base_predictions: Dict[str, np.ndarray], y: pd.Series) -> Dict[str, float]:
        """ì•ˆì •ì„± ìš”ì†Œ ê³„ì‚°"""
        stability_factors = {}
        
        for name, pred in base_predictions.items():
            try:
                # ì˜ˆì¸¡ê°’ ë³€ë™ì„±
                pred_std = np.std(pred)
                pred_var_score = min(pred_std * 10, 1.0)  # ì •ê·œí™”
                
                # CTR í¸í–¥ ì•ˆì •ì„±
                predicted_ctr = pred.mean()
                actual_ctr = y.mean()
                ctr_bias = abs(predicted_ctr - actual_ctr)
                ctr_stability = np.exp(-ctr_bias * 200)
                
                # ë¶„í¬ ì•ˆì •ì„± (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
                p = np.clip(pred, 1e-15, 1 - 1e-15)
                entropy = -np.mean(p * np.log2(p) + (1 - p) * np.log2(1 - p))
                entropy_stability = entropy / np.log2(2)  # ì •ê·œí™”
                
                # ì¢…í•© ì•ˆì •ì„±
                stability_score = 0.4 * ctr_stability + 0.3 * entropy_stability + 0.3 * pred_var_score
                stability_factors[name] = max(stability_score, 0.1)
                
            except Exception as e:
                logger.warning(f"{name} ì•ˆì •ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")
                stability_factors[name] = 0.5
        
        logger.info(f"ì•ˆì •ì„± ìš”ì†Œ: {stability_factors}")
        return stability_factors
    
    def _combine_weights_with_stability(self) -> Dict[str, float]:
        """ì„±ëŠ¥, ë‹¤ì–‘ì„±, ì•ˆì •ì„± ê°€ì¤‘ì¹˜ ê²°í•©"""
        
        combined_weights = {}
        model_names = list(self.model_weights.keys())
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        performance_sum = sum(self.model_weights.values())
        diversity_sum = sum(self.diversity_weights.values())
        stability_sum = sum(self.stability_factors.values())
        
        if performance_sum > 0 and diversity_sum > 0 and stability_sum > 0:
            for name in model_names:
                perf_weight = self.model_weights[name] / performance_sum
                div_weight = self.diversity_weights[name] / diversity_sum
                stab_weight = self.stability_factors[name] / stability_sum
                
                # ì„±ëŠ¥ 50%, ë‹¤ì–‘ì„± 25%, ì•ˆì •ì„± 25% ë¹„ìœ¨ë¡œ ê²°í•©
                combined_weight = 0.5 * perf_weight + 0.25 * div_weight + 0.25 * stab_weight
                combined_weights[name] = combined_weight
        else:
            # ê· ë“± ê°€ì¤‘ì¹˜
            combined_weights = {name: 1.0/len(model_names) for name in model_names}
        
        # ìµœì¢… ì •ê·œí™”
        total_weight = sum(combined_weights.values())
        if total_weight > 0:
            combined_weights = {k: v/total_weight for k, v in combined_weights.items()}
        
        return combined_weights
    
    def predict_proba(self, base_predictions: Dict[str, np.ndarray]) -> np.ndarray:
        """ì•ˆì •í™”ëœ ì•™ìƒë¸” ì˜ˆì¸¡"""
        if not self.is_fitted:
            raise ValueError("ì•™ìƒë¸” ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        ensemble_pred = np.zeros(len(list(base_predictions.values())[0]))
        
        for name, weight in self.final_weights.items():
            if name in base_predictions:
                ensemble_pred += weight * base_predictions[name]
        
        return ensemble_pred

class CTRMetaLearning(BaseEnsemble):
    """CTR ì˜ˆì¸¡ ë©”íƒ€ í•™ìŠµ ì•™ìƒë¸” - 1070ë§Œí–‰ ìµœì í™”"""
    
    def __init__(self, meta_model_type: str = 'ridge_advanced', use_meta_features: bool = True):
        super().__init__("CTRMetaLearning")
        self.meta_model_type = meta_model_type
        self.use_meta_features = use_meta_features
        self.meta_model = None
        self.feature_scaler = None
        self.feature_selector = None
        
    def fit(self, X: pd.DataFrame, y: pd.Series, base_predictions: Optional[Dict[str, np.ndarray]] = None):
        """ë©”íƒ€ í•™ìŠµ ì•™ìƒë¸” í•™ìŠµ - 1070ë§Œí–‰ ìµœì í™”"""
        logger.info(f"CTR ë©”íƒ€ í•™ìŠµ ì•™ìƒë¸” í•™ìŠµ ì‹œì‘ - ë©”íƒ€ëª¨ë¸: {self.meta_model_type}")
        
        # Out-of-fold ì˜ˆì¸¡ ìƒì„± (íš¨ìœ¨ì )
        oof_predictions = self._generate_oof_predictions_efficient(X, y)
        
        # ë©”íƒ€ í”¼ì²˜ ìƒì„±
        if self.use_meta_features:
            meta_features = self._create_meta_features_advanced(oof_predictions, X)
        else:
            meta_features = oof_predictions
        
        # ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
        self._train_meta_model_advanced(meta_features, y)
        
        self.is_fitted = True
        logger.info("CTR ë©”íƒ€ í•™ìŠµ ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ")
    
    def _generate_oof_predictions_efficient(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:
        """íš¨ìœ¨ì ì¸ Out-of-fold ì˜ˆì¸¡ ìƒì„± - 1070ë§Œí–‰ ìµœì í™”"""
        
        oof_predictions = pd.DataFrame(index=X.index)
        
        # 1070ë§Œí–‰ì—ì„œëŠ” 3í´ë“œë¡œ ì œí•œ
        tscv = TimeSeriesSplit(n_splits=3, gap=1000)
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œëŠ” ìƒ˜í”Œë§ ì ìš©
        if len(X) > 2000000:
            sample_size = 1500000
            sample_indices = np.random.choice(len(X), sample_size, replace=False)
            X_sample = X.iloc[sample_indices]
            y_sample = y.iloc[sample_indices]
            logger.info(f"ë©”íƒ€ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ìƒ˜í”Œë§: {len(X):,} â†’ {sample_size:,}")
        else:
            X_sample = X
            y_sample = y
        
        for model_name, model in self.base_models.items():
            oof_pred = np.zeros(len(X_sample))
            
            for fold, (train_idx, val_idx) in enumerate(tscv.split(X_sample)):
                try:
                    X_train_fold = X_sample.iloc[train_idx]
                    X_val_fold = X_sample.iloc[val_idx]
                    y_train_fold = y_sample.iloc[train_idx]
                    
                    # ëª¨ë¸ ë³µì‚¬ ë° í•™ìŠµ
                    fold_model = self._clone_model(model)
                    fold_model.fit(X_train_fold, y_train_fold)
                    
                    # Out-of-fold ì˜ˆì¸¡
                    oof_pred[val_idx] = fold_model.predict_proba(X_val_fold)
                    
                    logger.info(f"{model_name} í´ë“œ {fold + 1} ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"{model_name} í´ë“œ {fold} í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
                    oof_pred[val_idx] = self.target_ctr
            
            # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í™•ì¥ (ìƒ˜í”Œë§í•œ ê²½ìš°)
            if len(X) > len(X_sample):
                full_oof_pred = np.full(len(X), oof_pred.mean())
                full_oof_pred[sample_indices] = oof_pred
                oof_predictions[model_name] = full_oof_pred
            else:
                oof_predictions[model_name] = oof_pred
        
        return oof_predictions
    
    def _clone_model(self, model: BaseModel) -> BaseModel:
        """ëª¨ë¸ ë³µì‚¬"""
        from models import ModelFactory
        
        try:
            model_type = model.name.lower()
            if 'lightgbm' in model_type:
                model_type = 'lightgbm'
            elif 'xgboost' in model_type:
                model_type = 'xgboost'
            elif 'catboost' in model_type:
                model_type = 'catboost'
            elif 'deepctr' in model_type:
                return ModelFactory.create_model('deepctr', input_dim=100, params=model.params)
            else:
                model_type = 'logistic'
            
            return ModelFactory.create_model(model_type, params=model.params)
        except:
            return model
    
    def _create_meta_features_advanced(self, oof_predictions: pd.DataFrame, X: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ ë©”íƒ€ í”¼ì²˜ ìƒì„± - 1070ë§Œí–‰ ìµœì í™”"""
        
        meta_features = oof_predictions.copy()
        
        try:
            logger.info("ê³ ê¸‰ ë©”íƒ€ í”¼ì²˜ ìƒì„± ì‹œì‘")
            
            # ê¸°ë³¸ í†µê³„ í”¼ì²˜ (í–¥ìƒ)
            meta_features['pred_mean'] = oof_predictions.mean(axis=1)
            meta_features['pred_std'] = oof_predictions.std(axis=1)
            meta_features['pred_min'] = oof_predictions.min(axis=1)
            meta_features['pred_max'] = oof_predictions.max(axis=1)
            meta_features['pred_median'] = oof_predictions.median(axis=1)
            meta_features['pred_range'] = meta_features['pred_max'] - meta_features['pred_min']
            meta_features['pred_iqr'] = oof_predictions.quantile(0.75, axis=1) - oof_predictions.quantile(0.25, axis=1)
            
            # ê³ ê¸‰ í†µê³„ í”¼ì²˜
            meta_features['pred_skew'] = oof_predictions.skew(axis=1)
            meta_features['pred_kurtosis'] = oof_predictions.kurtosis(axis=1)
            
            # ìˆœìœ„ ê¸°ë°˜ í”¼ì²˜
            for col in oof_predictions.columns:
                meta_features[f'{col}_rank'] = oof_predictions[col].rank(pct=True)
            
            # ëª¨ë¸ê°„ ê´€ê³„ í”¼ì²˜ (ì„ íƒì )
            model_cols = oof_predictions.columns.tolist()
            if len(model_cols) <= 5:  # ëª¨ë¸ ìˆ˜ê°€ ì ì„ ë•Œë§Œ
                for i, col1 in enumerate(model_cols):
                    for col2 in model_cols[i+1:]:
                        meta_features[f'{col1}_{col2}_diff'] = oof_predictions[col1] - oof_predictions[col2]
                        meta_features[f'{col1}_{col2}_ratio'] = oof_predictions[col1] / (oof_predictions[col2] + 1e-8)
                        meta_features[f'{col1}_{col2}_avg'] = (oof_predictions[col1] + oof_predictions[col2]) / 2
            
            # ì‹ ë¢°ë„ ë° ì¼ê´€ì„± í”¼ì²˜
            meta_features['prediction_confidence'] = 1 - meta_features['pred_std']
            meta_features['consensus_strength'] = np.exp(-meta_features['pred_std'])
            meta_features['prediction_entropy'] = self._calculate_prediction_entropy(oof_predictions.values)
            
            # CTR íŠ¹í™” í”¼ì²˜
            meta_features['ctr_distance'] = np.abs(meta_features['pred_mean'] - self.target_ctr)
            meta_features['ctr_normalized'] = meta_features['pred_mean'] / self.target_ctr
            
            # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ í”¼ì²˜
            quantiles = [0.1, 0.25, 0.75, 0.9]
            for q in quantiles:
                meta_features[f'pred_q{int(q*100)}'] = oof_predictions.quantile(q, axis=1)
            
            logger.info(f"ê³ ê¸‰ ë©”íƒ€ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {meta_features.shape[1]}ê°œ í”¼ì²˜")
            
        except Exception as e:
            logger.warning(f"ê³ ê¸‰ ë©”íƒ€ í”¼ì²˜ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        
        return meta_features
    
    def _calculate_prediction_entropy(self, predictions: np.ndarray) -> np.ndarray:
        """ì˜ˆì¸¡ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            entropies = []
            for i in range(predictions.shape[0]):
                row_preds = predictions[i]
                p = np.clip(row_preds, 1e-15, 1 - 1e-15)
                entropy = -np.mean(p * np.log2(p) + (1 - p) * np.log2(1 - p))
                entropies.append(entropy)
            return np.array(entropies)
        except:
            return np.zeros(predictions.shape[0])
    
    def _train_meta_model_advanced(self, meta_features: pd.DataFrame, y: pd.Series):
        """ê³ ê¸‰ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ - 1070ë§Œí–‰ ìµœì í™”"""
        
        # í”¼ì²˜ ì „ì²˜ë¦¬
        meta_features_clean = meta_features.fillna(0)
        meta_features_clean = meta_features_clean.replace([np.inf, -np.inf], [1e6, -1e6])
        
        # í”¼ì²˜ ì„ íƒ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ì¤‘ìš”)
        if meta_features_clean.shape[1] > 50:
            from sklearn.feature_selection import SelectKBest, f_regression
            self.feature_selector = SelectKBest(score_func=f_regression, k=min(50, meta_features_clean.shape[1]))
            meta_features_selected = self.feature_selector.fit_transform(meta_features_clean, y)
            logger.info(f"í”¼ì²˜ ì„ íƒ: {meta_features_clean.shape[1]} â†’ {meta_features_selected.shape[1]}")
        else:
            meta_features_selected = meta_features_clean.values
            self.feature_selector = None
        
        # ìŠ¤ì¼€ì¼ë§
        self.feature_scaler = StandardScaler()
        meta_features_scaled = self.feature_scaler.fit_transform(meta_features_selected)
        
        # ë©”íƒ€ ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
        if self.meta_model_type == 'ridge_advanced':
            self.meta_model = RidgeCV(
                alphas=[0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], 
                cv=3,
                scoring='neg_mean_squared_error'
            )
        elif self.meta_model_type == 'logistic_advanced':
            self.meta_model = LogisticRegression(
                random_state=42, 
                max_iter=2000,
                class_weight={0: 1, 1: 49.75},  # ì‹¤ì œ CTR ë°˜ì˜
                solver='lbfgs',
                C=0.1
            )
        elif self.meta_model_type == 'mlp_advanced':
            self.meta_model = MLPRegressor(
                hidden_layer_sizes=(200, 100, 50),
                max_iter=2000,
                random_state=42,
                early_stopping=True,
                validation_fraction=0.2,
                alpha=0.01
            )
        else:
            self.meta_model = RidgeCV(cv=3)
        
        try:
            if 'logistic' in self.meta_model_type:
                self.meta_model.fit(meta_features_scaled, y)
            else:
                self.meta_model.fit(meta_features_scaled, y)
            
            # ì„±ëŠ¥ í‰ê°€
            meta_pred = self.meta_model.predict(meta_features_scaled)
            if 'logistic' in self.meta_model_type:
                meta_pred = self.meta_model.predict_proba(meta_features_scaled)[:, 1]
            
            # Combined Scoreë¡œ í‰ê°€
            combined_score = self.metrics_calculator.combined_score(y, meta_pred)
            logger.info(f"ë©”íƒ€ ëª¨ë¸ Combined Score: {combined_score:.4f}")
            
            if combined_score >= 0.30:
                logger.info(f"ğŸ¯ ë©”íƒ€ ëª¨ë¸ ëª©í‘œ ë‹¬ì„±!")
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def predict_proba(self, base_predictions: Dict[str, np.ndarray]) -> np.ndarray:
        """ë©”íƒ€ í•™ìŠµ ì˜ˆì¸¡ - 1070ë§Œí–‰ ìµœì í™”"""
        if not self.is_fitted:
            raise ValueError("ì•™ìƒë¸” ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # ê¸°ë³¸ ì˜ˆì¸¡ì„ DataFrameìœ¼ë¡œ ë³€í™˜
            pred_df = pd.DataFrame(base_predictions)
            
            # ë©”íƒ€ í”¼ì²˜ ìƒì„±
            if self.use_meta_features:
                meta_features = self._create_inference_meta_features_advanced(pred_df)
            else:
                meta_features = pred_df
            
            # ì „ì²˜ë¦¬
            meta_features_clean = meta_features.fillna(0)
            meta_features_clean = meta_features_clean.replace([np.inf, -np.inf], [1e6, -1e6])
            
            # í”¼ì²˜ ì„ íƒ
            if self.feature_selector is not None:
                meta_features_selected = self.feature_selector.transform(meta_features_clean)
            else:
                meta_features_selected = meta_features_clean.values
            
            # ìŠ¤ì¼€ì¼ë§
            meta_features_scaled = self.feature_scaler.transform(meta_features_selected)
            
            # ì˜ˆì¸¡
            if 'logistic' in self.meta_model_type:
                ensemble_pred = self.meta_model.predict_proba(meta_features_scaled)[:, 1]
            else:
                ensemble_pred = self.meta_model.predict(meta_features_scaled)
                ensemble_pred = np.clip(ensemble_pred, 0.001, 0.999)
            
            return ensemble_pred
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ í•™ìŠµ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return np.mean(list(base_predictions.values()), axis=0)
    
    def _create_inference_meta_features_advanced(self, pred_df: pd.DataFrame) -> pd.DataFrame:
        """ì¶”ë¡ ìš© ê³ ê¸‰ ë©”íƒ€ í”¼ì²˜ ìƒì„±"""
        
        meta_features = pred_df.copy()
        
        try:
            # ê¸°ë³¸ í†µê³„ í”¼ì²˜
            meta_features['pred_mean'] = pred_df.mean(axis=1)
            meta_features['pred_std'] = pred_df.std(axis=1)
            meta_features['pred_min'] = pred_df.min(axis=1)
            meta_features['pred_max'] = pred_df.max(axis=1)
            meta_features['pred_median'] = pred_df.median(axis=1)
            meta_features['pred_range'] = meta_features['pred_max'] - meta_features['pred_min']
            meta_features['pred_iqr'] = pred_df.quantile(0.75, axis=1) - pred_df.quantile(0.25, axis=1)
            
            # ê³ ê¸‰ í†µê³„ í”¼ì²˜
            meta_features['pred_skew'] = pred_df.skew(axis=1)
            meta_features['pred_kurtosis'] = pred_df.kurtosis(axis=1)
            
            # ìˆœìœ„ ê¸°ë°˜ í”¼ì²˜
            for col in pred_df.columns:
                meta_features[f'{col}_rank'] = pred_df[col].rank(pct=True)
            
            # ëª¨ë¸ê°„ ê´€ê³„ í”¼ì²˜ (ì„ íƒì )
            model_cols = pred_df.columns.tolist()
            if len(model_cols) <= 5:
                for i, col1 in enumerate(model_cols):
                    for col2 in model_cols[i+1:]:
                        meta_features[f'{col1}_{col2}_diff'] = pred_df[col1] - pred_df[col2]
                        meta_features[f'{col1}_{col2}_ratio'] = pred_df[col1] / (pred_df[col2] + 1e-8)
                        meta_features[f'{col1}_{col2}_avg'] = (pred_df[col1] + pred_df[col2]) / 2
            
            # ì‹ ë¢°ë„ í”¼ì²˜
            meta_features['prediction_confidence'] = 1 - meta_features['pred_std']
            meta_features['consensus_strength'] = np.exp(-meta_features['pred_std'])
            meta_features['prediction_entropy'] = self._calculate_prediction_entropy(pred_df.values)
            
            # CTR íŠ¹í™” í”¼ì²˜
            meta_features['ctr_distance'] = np.abs(meta_features['pred_mean'] - self.target_ctr)
            meta_features['ctr_normalized'] = meta_features['pred_mean'] / self.target_ctr
            
            # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ í”¼ì²˜
            quantiles = [0.1, 0.25, 0.75, 0.9]
            for q in quantiles:
                meta_features[f'pred_q{int(q*100)}'] = pred_df.quantile(q, axis=1)
            
        except Exception as e:
            logger.warning(f"ì¶”ë¡ ìš© ê³ ê¸‰ ë©”íƒ€ í”¼ì²˜ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        
        return meta_features

class CTREnsembleManager:
    """CTR íŠ¹í™” ì•™ìƒë¸” ê´€ë¦¬ í´ë˜ìŠ¤ - Combined Score 0.30+ ëª©í‘œ"""
    
    def __init__(self, config: Config = Config):
        self.config = config
        self.ensembles = {}
        self.base_models = {}
        self.best_ensemble = None
        self.ensemble_results = {}
        self.metrics_calculator = CTRMetrics()
        self.calibrated_ensemble = None
        self.optimal_ensemble = None
        self.target_score = 0.30
        
    def add_base_model(self, name: str, model: BaseModel):
        """ê¸°ë³¸ ëª¨ë¸ ì¶”ê°€"""
        self.base_models[name] = model
        logger.info(f"ê¸°ë³¸ ëª¨ë¸ ì¶”ê°€: {name}")
    
    def create_ensemble(self, ensemble_type: str, **kwargs) -> BaseEnsemble:
        """CTR íŠ¹í™” ì•™ìƒë¸” ìƒì„± - Combined Score 0.30+ ëª©í‘œ"""
        
        if ensemble_type == 'optimal':
            target_ctr = kwargs.get('target_ctr', 0.0201)
            optimization_method = kwargs.get('optimization_method', 'combined_plus')
            ensemble = CTROptimalEnsemble(target_ctr, optimization_method)
            self.optimal_ensemble = ensemble
        
        elif ensemble_type == 'stabilized':
            diversification_method = kwargs.get('diversification_method', 'rank_weighted_advanced')
            ensemble = CTRStabilizedEnsemble(diversification_method)
        
        elif ensemble_type == 'meta':
            meta_model_type = kwargs.get('meta_model_type', 'ridge_advanced')
            use_meta_features = kwargs.get('use_meta_features', True)
            ensemble = CTRMetaLearning(meta_model_type, use_meta_features)
        
        elif ensemble_type == 'weighted':
            weights = kwargs.get('weights', None)
            ensemble = CTRWeightedBlending(weights)
        
        elif ensemble_type == 'calibrated':
            target_ctr = kwargs.get('target_ctr', 0.0201)
            calibration_method = kwargs.get('calibration_method', 'platt')
            ensemble = CTRCalibratedEnsemble(target_ctr, calibration_method)
            self.calibrated_ensemble = ensemble
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•™ìƒë¸” íƒ€ì…: {ensemble_type}")
        
        # ê¸°ë³¸ ëª¨ë¸ ì¶”ê°€
        for name, model in self.base_models.items():
            ensemble.add_base_model(name, model)
        
        self.ensembles[ensemble_type] = ensemble
        logger.info(f"ì•™ìƒë¸” ìƒì„±: {ensemble_type} (Combined Score 0.30+ ëª©í‘œ)")
        
        return ensemble
    
    def train_all_ensembles(self, X: pd.DataFrame, y: pd.Series):
        """ëª¨ë“  ì•™ìƒë¸” í•™ìŠµ - Combined Score 0.30+ ëª©í‘œ"""
        logger.info("ëª¨ë“  ì•™ìƒë¸” í•™ìŠµ ì‹œì‘ (Combined Score 0.30+ ëª©í‘œ)")
        
        # ê¸°ë³¸ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘ (ëŒ€ìš©ëŸ‰ ìµœì í™”)
        base_predictions = {}
        for name, model in self.base_models.items():
            try:
                logger.info(f"{name} ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘ ì‹œì‘")
                pred = model.predict_proba(X)
                base_predictions[name] = pred
                
                # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ì²´í¬
                combined_score = self.metrics_calculator.combined_score(y, pred)
                logger.info(f"{name} ê°œë³„ ì„±ëŠ¥ - Combined Score: {combined_score:.4f}")
                
                if combined_score >= 0.30:
                    logger.info(f"ğŸ¯ {name} ê°œë³„ ëª¨ë¸ì´ ëª©í‘œ ë‹¬ì„±!")
                
            except Exception as e:
                logger.error(f"{name} ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
                base_predictions[name] = np.full(len(X), 0.0201)
        
        # ê° ì•™ìƒë¸” í•™ìŠµ
        successful_ensembles = 0
        target_achieved_ensembles = []
        
        for ensemble_type, ensemble in self.ensembles.items():
            try:
                logger.info(f"{ensemble_type} ì•™ìƒë¸” í•™ìŠµ ì‹œì‘")
                start_time = time.time()
                
                ensemble.fit(X, y, base_predictions)
                
                training_time = time.time() - start_time
                successful_ensembles += 1
                
                logger.info(f"{ensemble_type} ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {training_time:.2f}ì´ˆ)")
                
            except Exception as e:
                logger.error(f"{ensemble_type} ì•™ìƒë¸” í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
        
        logger.info(f"ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ - ì„±ê³µ: {successful_ensembles}/{len(self.ensembles)}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
    
    def evaluate_ensembles(self, X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, float]:
        """ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ - Combined Score 0.30+ ëª©í‘œ"""
        logger.info("ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ ì‹œì‘ (Combined Score 0.30+ ëª©í‘œ)")
        
        results = {}
        target_achieved_count = 0
        
        # ê¸°ë³¸ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘
        base_predictions = {}
        for name, model in self.base_models.items():
            try:
                pred = model.predict_proba(X_val)
                base_predictions[name] = pred
                
                score = self.metrics_calculator.combined_score(y_val, pred)
                results[f"base_{name}"] = score
                
                if score >= self.target_score:
                    target_achieved_count += 1
                    logger.info(f"ğŸ¯ {name} ê¸°ë³¸ ëª¨ë¸ ëª©í‘œ ë‹¬ì„±: {score:.4f}")
                else:
                    logger.info(f"{name} ê¸°ë³¸ ëª¨ë¸: {score:.4f}")
                
            except Exception as e:
                logger.error(f"{name} ëª¨ë¸ ê²€ì¦ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
                results[f"base_{name}"] = 0.0
        
        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        best_ensemble_score = 0.0
        best_ensemble_name = None
        
        for ensemble_type, ensemble in self.ensembles.items():
            if ensemble.is_fitted:
                try:
                    ensemble_pred = ensemble.predict_proba(base_predictions)
                    score = self.metrics_calculator.combined_score(y_val, ensemble_pred)
                    results[f"ensemble_{ensemble_type}"] = score
                    
                    # CTR ë¶„ì„
                    predicted_ctr = ensemble_pred.mean()
                    actual_ctr = y_val.mean()
                    ctr_bias = abs(predicted_ctr - actual_ctr)
                    
                    if score >= self.target_score:
                        target_achieved_count += 1
                        logger.info(f"ğŸ¯ {ensemble_type} ì•™ìƒë¸” ëª©í‘œ ë‹¬ì„±: {score:.4f} (CTR í¸í–¥: {ctr_bias:.4f})")
                    else:
                        logger.info(f"{ensemble_type} ì•™ìƒë¸”: {score:.4f} (CTR í¸í–¥: {ctr_bias:.4f})")
                    
                    if score > best_ensemble_score:
                        best_ensemble_score = score
                        best_ensemble_name = ensemble_type
                    
                except Exception as e:
                    logger.error(f"{ensemble_type} ì•™ìƒë¸” í‰ê°€ ì‹¤íŒ¨: {str(e)}")
                    results[f"ensemble_{ensemble_type}"] = 0.0
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸/ì•™ìƒë¸” ì„ íƒ
        if results:
            best_name = max(results, key=results.get)
            best_score = results[best_name]
            
            if best_name.startswith('ensemble_'):
                ensemble_type = best_name.replace('ensemble_', '')
                self.best_ensemble = self.ensembles[ensemble_type]
                logger.info(f"ìµœê³  ì„±ëŠ¥ ì•™ìƒë¸”: {ensemble_type} (Combined Score: {best_score:.4f})")
            else:
                logger.info(f"ê¸°ë³¸ ëª¨ë¸ì´ ìš°ìˆ˜í•¨: {best_name} (Combined Score: {best_score:.4f})")
                self.best_ensemble = None
            
            # ëª©í‘œ ë‹¬ì„± ìš”ì•½
            total_models = len(self.base_models) + len([e for e in self.ensembles.values() if e.is_fitted])
            logger.info(f"Combined Score 0.30+ ë‹¬ì„±: {target_achieved_count}/{total_models}")
            
            if best_score >= self.target_score:
                logger.info(f"ğŸ¯ ìµœì¢… ëª©í‘œ ë‹¬ì„±! ìµœê³  ì ìˆ˜: {best_score:.4f}")
            else:
                logger.warning(f"âš ï¸ ìµœì¢… ëª©í‘œ ë¯¸ë‹¬ì„±. ìµœê³  ì ìˆ˜: {best_score:.4f}")
        
        self.ensemble_results = results
        return results
    
    def predict_with_best_ensemble(self, X: pd.DataFrame) -> np.ndarray:
        """ìµœê³  ì„±ëŠ¥ ì•™ìƒë¸”ë¡œ ì˜ˆì¸¡"""
        
        # ê¸°ë³¸ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜ì§‘
        base_predictions = {}
        for name, model in self.base_models.items():
            try:
                pred = model.predict_proba(X)
                base_predictions[name] = pred
            except Exception as e:
                logger.error(f"{name} ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
                base_predictions[name] = np.full(len(X), 0.0201)
        
        if self.best_ensemble is None:
            # ê¸°ë³¸ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ ì„ íƒ
            best_model_name = None
            best_score = 0
            
            for result_name, score in self.ensemble_results.items():
                if result_name.startswith('base_') and score > best_score:
                    best_score = score
                    best_model_name = result_name.replace('base_', '')
            
            if best_model_name and best_model_name in self.base_models:
                logger.info(f"ìµœê³  ì„±ëŠ¥ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {best_model_name} (Score: {best_score:.4f})")
                return self.base_models[best_model_name].predict_proba(X)
            else:
                # í‰ê·  ì•™ìƒë¸”
                return np.mean(list(base_predictions.values()), axis=0)
        
        return self.best_ensemble.predict_proba(base_predictions)
    
    def save_ensembles(self, output_dir: Path = None):
        """ì•™ìƒë¸” ì €ì¥"""
        if output_dir is None:
            output_dir = self.config.MODEL_DIR
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ê°œë³„ ì•™ìƒë¸” ì €ì¥
        for ensemble_type, ensemble in self.ensembles.items():
            if ensemble.is_fitted:
                ensemble_path = output_dir / f"ensemble_{ensemble_type}.pkl"
                
                try:
                    with open(ensemble_path, 'wb') as f:
                        pickle.dump(ensemble, f)
                    
                    logger.info(f"{ensemble_type} ì•™ìƒë¸” ì €ì¥: {ensemble_path}")
                except Exception as e:
                    logger.error(f"{ensemble_type} ì•™ìƒë¸” ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        # ìµœê³  ì•™ìƒë¸” ì •ë³´ ì €ì¥
        best_info = {
            'best_ensemble_type': self.best_ensemble.name if self.best_ensemble else None,
            'ensemble_results': self.ensemble_results,
            'target_score': self.target_score,
            'target_achieved': any(score >= self.target_score for score in self.ensemble_results.values()),
            'calibrated_available': self.calibrated_ensemble is not None,
            'optimal_available': self.optimal_ensemble is not None,
            'training_data_size': getattr(self.best_ensemble, 'training_data_size', 0) if self.best_ensemble else 0
        }
        
        try:
            import json
            info_path = output_dir / "best_ensemble_info.json"
            with open(info_path, 'w') as f:
                json.dump(best_info, f, indent=2, default=str)
            logger.info(f"ì•™ìƒë¸” ì •ë³´ ì €ì¥: {info_path}")
        except Exception as e:
            logger.error(f"ì•™ìƒë¸” ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def load_ensembles(self, input_dir: Path = None):
        """ì•™ìƒë¸” ë¡œë”©"""
        if input_dir is None:
            input_dir = self.config.MODEL_DIR
        
        input_dir = Path(input_dir)
        
        ensemble_files = list(input_dir.glob("ensemble_*.pkl"))
        
        for ensemble_file in ensemble_files:
            try:
                ensemble_type = ensemble_file.stem.replace('ensemble_', '')
                
                with open(ensemble_file, 'rb') as f:
                    ensemble = pickle.load(f)
                
                self.ensembles[ensemble_type] = ensemble
                
                # íŠ¹ìˆ˜ ì•™ìƒë¸” ì°¸ì¡° ì„¤ì •
                if ensemble_type == 'calibrated':
                    self.calibrated_ensemble = ensemble
                elif ensemble_type == 'optimal':
                    self.optimal_ensemble = ensemble
                
                logger.info(f"{ensemble_type} ì•™ìƒë¸” ë¡œë”© ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"{ensemble_file} ì•™ìƒë¸” ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        
        # ìµœê³  ì•™ìƒë¸” ì •ë³´ ë¡œë”©
        info_path = input_dir / "best_ensemble_info.json"
        if info_path.exists():
            try:
                import json
                with open(info_path, 'r') as f:
                    best_info = json.load(f)
                
                best_type = best_info.get('best_ensemble_type')
                if best_type:
                    for ensemble in self.ensembles.values():
                        if ensemble.name == best_type:
                            self.best_ensemble = ensemble
                            break
                
                self.ensemble_results = best_info.get('ensemble_results', {})
                self.target_score = best_info.get('target_score', 0.30)
                
            except Exception as e:
                logger.error(f"ì•™ìƒë¸” ì •ë³´ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
    
    def get_ensemble_summary(self) -> Dict[str, Any]:
        """ì•™ìƒë¸” ìš”ì•½ ì •ë³´ - Combined Score 0.30+ ëª©í‘œ"""
        
        target_achieved_count = sum(1 for score in self.ensemble_results.values() if score >= self.target_score)
        best_score = max(self.ensemble_results.values()) if self.ensemble_results else 0.0
        
        return {
            'total_ensembles': len(self.ensembles),
            'fitted_ensembles': sum(1 for e in self.ensembles.values() if e.is_fitted),
            'best_ensemble': self.best_ensemble.name if self.best_ensemble else None,
            'ensemble_results': self.ensemble_results,
            'base_models_count': len(self.base_models),
            'calibrated_ensemble_available': self.calibrated_ensemble is not None and self.calibrated_ensemble.is_fitted,
            'optimal_ensemble_available': self.optimal_ensemble is not None and self.optimal_ensemble.is_fitted,
            'ensemble_types': list(self.ensembles.keys()),
            'target_score': self.target_score,
            'target_achieved_count': target_achieved_count,
            'target_achieved': target_achieved_count > 0,
            'best_score': best_score,
            'goal_reached': best_score >= self.target_score
        }

# ê¸°ì¡´ ì•™ìƒë¸” í´ë˜ìŠ¤ë“¤ (í•˜ìœ„ í˜¸í™˜ì„±)
class CTRCalibratedEnsemble(BaseEnsemble):
    """CTR ë³´ì • ì•™ìƒë¸” í´ë˜ìŠ¤ - 1070ë§Œí–‰ ìµœì í™”"""
    
    def __init__(self, target_ctr: float = 0.0201, calibration_method: str = 'platt'):
        super().__init__("CTRCalibratedEnsemble")
        self.target_ctr = target_ctr
        self.calibration_method = calibration_method
        self.weights = {}
        self.calibrator = None
        self.bias_correction = 0.0
        self.metrics_calculator = CTRMetrics()
        
    def fit(self, X: pd.DataFrame, y: pd.Series, base_predictions: Dict[str, np.ndarray]):
        """ë³´ì •ëœ ì•™ìƒë¸” í•™ìŠµ - Combined Score 0.30+ ëª©í‘œ"""
        logger.info("CTR ë³´ì • ì•™ìƒë¸” í•™ìŠµ ì‹œì‘")
        
        available_models = list(base_predictions.keys())
        
        if len(available_models) < 2:
            if available_models:
                self.weights = {available_models[0]: 1.0}
            self.is_fitted = True
            return
        
        self.training_data_size = len(X)
        
        # Combined Score ê¸°ì¤€ ê°€ì¤‘ì¹˜ ìµœì í™”
        self.weights = self._optimize_weights_for_combined_score_advanced(base_predictions, y)
        
        # ê°€ì¤‘ ì•™ìƒë¸” ìƒì„±
        ensemble_pred = self._create_weighted_ensemble(base_predictions)
        
        # CTR ë³´ì • ì ìš©
        self._apply_ctr_calibration_advanced(ensemble_pred, y)
        
        self.is_fitted = True
        
        # ì„±ëŠ¥ ê²€ì¦
        final_score = self.metrics_calculator.combined_score(y, self.predict_proba(base_predictions))
        if final_score >= 0.30:
            logger.info(f"ğŸ¯ CTR ë³´ì • ì•™ìƒë¸” ëª©í‘œ ë‹¬ì„±: {final_score:.4f}")
        else:
            logger.warning(f"CTR ë³´ì • ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ: {final_score:.4f}")
    
    def _optimize_weights_for_combined_score_advanced(self, base_predictions: Dict[str, np.ndarray], y: pd.Series) -> Dict[str, float]:
        """Combined Score ìµœì í™”ë¥¼ ìœ„í•œ ê³ ê¸‰ ê°€ì¤‘ì¹˜ íŠœë‹"""
        
        model_names = list(base_predictions.keys())
        
        if not OPTUNA_AVAILABLE:
            # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            weights = {}
            for name, pred in base_predictions.items():
                score = self.metrics_calculator.combined_score(y, pred)
                weights[name] = max(score, 0.01)
            
            total_weight = sum(weights.values())
            if total_weight > 0:
                weights = {k: v/total_weight for k, v in weights.items()}
            
            return weights
        
        def advanced_objective(trial):
            weights = {}
            for name in model_names:
                weights[name] = trial.suggest_float(f'weight_{name}', 0.01, 0.99)
            
            total_weight = sum(weights.values())
            if total_weight == 0:
                return 0.0
            
            weights = {k: v/total_weight for k, v in weights.items()}
            
            ensemble_pred = np.zeros(len(y))
            for name, weight in weights.items():
                if name in base_predictions:
                    ensemble_pred += weight * base_predictions[name]
            
            score = self.metrics_calculator.combined_score(y, ensemble_pred)
            
            # Combined Score 0.30+ ëª©í‘œì— ëŒ€í•œ ë³´ë„ˆìŠ¤
            if score >= 0.30:
                bonus = (score - 0.30) * 20
                return score + bonus
            
            return score
        
        try:
            study = optuna.create_study(
                direction='maximize',
                sampler=TPESampler(seed=42, n_startup_trials=15),
                pruner=MedianPruner(n_startup_trials=10)
            )
            study.optimize(advanced_objective, n_trials=150, show_progress_bar=False)
            
            optimized_weights = {}
            for param_name, weight in study.best_params.items():
                model_name = param_name.replace('weight_', '')
                optimized_weights[model_name] = weight
            
            total_weight = sum(optimized_weights.values())
            if total_weight > 0:
                optimized_weights = {k: v/total_weight for k, v in optimized_weights.items()}
            
            logger.info(f"ê³ ê¸‰ ê°€ì¤‘ì¹˜ ìµœì í™” ì™„ë£Œ - ì ìˆ˜: {study.best_value:.4f}")
            return optimized_weights
            
        except Exception as e:
            logger.error(f"ê³ ê¸‰