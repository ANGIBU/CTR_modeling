# models.py

import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, Tuple, List
import logging
from abc import ABC, abstractmethod
import pickle
import gc
import warnings
warnings.filterwarnings('ignore')

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    logging.warning("LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    logging.warning("XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    logging.warning("CatBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

TORCH_AVAILABLE = False
AMP_AVAILABLE = False
torch = None
nn = None
optim = None
DataLoader = None
TensorDataset = None
GradScaler = None
autocast = None

try:
    import torch
    
    gpu_available = False
    if torch.cuda.is_available():
        try:
            test_tensor = torch.zeros(2000, 2000).cuda()
            test_result = test_tensor.sum()
            del test_tensor
            torch.cuda.empty_cache()
            gpu_available = True
        except Exception as e:
            logging.warning(f"GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}. CPU ì „ìš© ëª¨ë“œ")
            gpu_available = False
    
    TORCH_AVAILABLE = True
    
    if TORCH_AVAILABLE:
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import DataLoader as TorchDataLoader, TensorDataset
        
        try:
            if gpu_available and hasattr(torch.cuda, 'amp'):
                from torch.cuda.amp import GradScaler, autocast
                AMP_AVAILABLE = True
            else:
                AMP_AVAILABLE = False
        except (ImportError, AttributeError):
            AMP_AVAILABLE = False
            
except ImportError:
    TORCH_AVAILABLE = False
    AMP_AVAILABLE = False
    logging.warning("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DeepCTR ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

try:
    from sklearn.calibration import CalibratedClassifierCV
    from sklearn.isotonic import IsotonicRegression
    from sklearn.linear_model import LogisticRegression
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logging.warning("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import roc_auc_score, log_loss
except ImportError:
    pass

from config import Config

logger = logging.getLogger(__name__)

class BaseModel(ABC):
    """ëª¨ë“  ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤ - CTR ìµœì í™”"""
    
    def __init__(self, name: str, params: Dict[str, Any] = None):
        self.name = name
        self.params = params or {}
        self.model = None
        self.is_fitted = False
        self.feature_names = None
        self.calibrator = None
        self.is_calibrated = False
        self.training_data_size = 0
        self.target_ctr = 0.0201  # ì‹¤ì œ CTR
        
    @abstractmethod
    def fit(self, X_train: pd.DataFrame, y_train: pd.Series, 
            X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """ëª¨ë¸ í•™ìŠµ"""
        pass
    
    @abstractmethod
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """í™•ë¥  ì˜ˆì¸¡"""
        pass
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """ì´ì§„ ì˜ˆì¸¡"""
        proba = self.predict_proba(X)
        return (proba >= 0.5).astype(int)
    
    def apply_calibration(self, X_train: pd.DataFrame, y_train: pd.Series, 
                         method: str = 'platt', cv_folds: int = 3):
        """ì˜ˆì¸¡ í™•ë¥  ë³´ì • ì ìš© - CTR íŠ¹í™”"""
        if not SKLEARN_AVAILABLE:
            logger.warning("scikit-learnì´ ì—†ì–´ calibrationì„ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
            
        try:
            train_pred = self.predict_proba_raw(X_train)
            
            if method == 'platt':
                calibrator = CalibratedClassifierCV(
                    estimator=None, 
                    method='sigmoid', 
                    cv=cv_folds
                )
            elif method == 'isotonic':
                calibrator = CalibratedClassifierCV(
                    estimator=None, 
                    method='isotonic', 
                    cv=cv_folds
                )
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” calibration ë°©ë²•: {method}")
                return
            
            calibrator.fit(train_pred.reshape(-1, 1), y_train)
            self.calibrator = calibrator
            self.is_calibrated = True
            
            # CTR í¸í–¥ í™•ì¸
            calibrated_pred = calibrator.predict_proba(train_pred.reshape(-1, 1))[:, 1]
            original_ctr = train_pred.mean()
            calibrated_ctr = calibrated_pred.mean()
            actual_ctr = y_train.mean()
            
            logger.info(f"{self.name} {method} calibration ì ìš© ì™„ë£Œ")
            logger.info(f"CTR ë³€í™”: {original_ctr:.4f} â†’ {calibrated_ctr:.4f} (ì‹¤ì œ: {actual_ctr:.4f})")
            
        except Exception as e:
            logger.error(f"Calibration ì ìš© ì‹¤íŒ¨ ({self.name}): {str(e)}")
    
    def predict_proba_raw(self, X: pd.DataFrame) -> np.ndarray:
        """ë³´ì •ë˜ì§€ ì•Šì€ ì›ë³¸ í™•ë¥  ì˜ˆì¸¡"""
        return self.predict_proba(X)
    
    def _ensure_feature_consistency(self, X: pd.DataFrame) -> pd.DataFrame:
        """í”¼ì²˜ ì¼ê´€ì„± ë³´ì¥"""
        if self.feature_names is None:
            return X
        
        try:
            for feature in self.feature_names:
                if feature not in X.columns:
                    X[feature] = 0.0
            
            X = X[self.feature_names]
            return X
        except Exception as e:
            logger.warning(f"í”¼ì²˜ ì¼ê´€ì„± ë³´ì¥ ì‹¤íŒ¨: {str(e)}")
            return X

class LightGBMModel(BaseModel):
    """CTR íŠ¹í™” LightGBM ëª¨ë¸ - 1070ë§Œí–‰ ìµœì í™”"""
    
    def __init__(self, params: Dict[str, Any] = None):
        if not LIGHTGBM_AVAILABLE:
            raise ImportError("LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # 1070ë§Œí–‰ ìµœì í™” ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        default_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 511,
            'learning_rate': 0.05,
            'feature_fraction': 0.75,
            'bagging_fraction': 0.65,
            'bagging_freq': 3,
            'min_child_samples': 500,
            'min_child_weight': 15,
            'lambda_l1': 4.0,
            'lambda_l2': 4.0,
            'max_depth': 15,
            'verbose': -1,
            'random_state': Config.RANDOM_STATE,
            'n_estimators': 5000,
            'early_stopping_rounds': 250,
            'scale_pos_weight': 49.75,  # ì‹¤ì œ CTR 0.0201 ë°˜ì˜
            'force_row_wise': True,
            'max_bin': 255,
            'num_threads': 12,  # Ryzen 5 5600X 12ìŠ¤ë ˆë“œ
            'device_type': 'cpu',
            'min_data_in_leaf': 200,
            'feature_fraction_bynode': 0.75,
            'extra_trees': True,
            'path_smooth': 1.5,
            'grow_policy': 'lossguide',
            'boost_from_average': True,
            'feature_pre_filter': False,
            'is_provide_training_metric': False,
            'cat_l2': 10.0,
            'cat_smooth': 10.0,
            'min_gain_to_split': 0.0,
            'reg_sqrt': False
        }
        
        if params:
            default_params.update(params)
        
        default_params = self._validate_large_data_params(default_params)
        
        super().__init__("LightGBM", default_params)
    
    def _validate_large_data_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """LightGBM ëŒ€ìš©ëŸ‰ ë°ì´í„° íŒŒë¼ë¯¸í„° ê²€ì¦ ê°•í™”"""
        safe_params = params.copy()
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì„¤ì •
        if 'objective' not in safe_params:
            safe_params['objective'] = 'binary'
        if 'metric' not in safe_params:
            safe_params['metric'] = 'binary_logloss'
        if 'verbose' not in safe_params:
            safe_params['verbose'] = -1
        
        # ì¶©ëŒ ë°©ì§€
        if 'is_unbalance' in safe_params and 'scale_pos_weight' in safe_params:
            safe_params.pop('is_unbalance', None)
        
        # 1070ë§Œí–‰ íŠ¹í™” ë²”ìœ„ ì œí•œ
        safe_params['num_leaves'] = min(max(safe_params.get('num_leaves', 511), 63), 2047)
        safe_params['max_bin'] = min(safe_params.get('max_bin', 255), 255)
        safe_params['num_threads'] = min(safe_params.get('num_threads', 12), 12)
        safe_params['max_depth'] = min(max(safe_params.get('max_depth', 15), -1), 20)
        
        # CTR íŠ¹í™” ì •ê·œí™” ê°•í™”
        safe_params['lambda_l1'] = max(safe_params.get('lambda_l1', 4.0), 2.0)
        safe_params['lambda_l2'] = max(safe_params.get('lambda_l2', 4.0), 2.0)
        safe_params['min_child_samples'] = max(safe_params.get('min_child_samples', 500), 100)
        safe_params['min_child_weight'] = max(safe_params.get('min_child_weight', 15), 5)
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì„±ëŠ¥ ìµœì í™”
        safe_params['force_row_wise'] = True
        safe_params['device_type'] = 'cpu'
        safe_params['boost_from_average'] = True
        safe_params['feature_pre_filter'] = False
        
        return safe_params
    
    def fit(self, X_train: pd.DataFrame, y_train: pd.Series, 
            X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """CTR íŠ¹í™” LightGBM ëª¨ë¸ í•™ìŠµ - 1070ë§Œí–‰ ìµœì í™”"""
        logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë°ì´í„° í¬ê¸°: {len(X_train):,}í–‰)")
        
        try:
            self.feature_names = list(X_train.columns)
            self.training_data_size = len(X_train)
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            if X_train.isnull().sum().sum() > 0:
                logger.warning("í•™ìŠµ ë°ì´í„°ì— ê²°ì¸¡ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                X_train = X_train.fillna(0)
            
            if X_val is not None and X_val.isnull().sum().sum() > 0:
                logger.warning("ê²€ì¦ ë°ì´í„°ì— ê²°ì¸¡ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤. 0ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                X_val = X_val.fillna(0)
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in X_train.columns:
                if X_train[col].dtype in ['float64']:
                    X_train[col] = X_train[col].astype('float32')
                if X_val is not None and X_val[col].dtype in ['float64']:
                    X_val[col] = X_val[col].astype('float32')
            
            # LightGBM Dataset ìƒì„± - ëŒ€ìš©ëŸ‰ ìµœì í™”
            train_data = lgb.Dataset(
                X_train, 
                label=y_train, 
                free_raw_data=False,
                params={'bin_construct_sample_cnt': 200000}  # ëŒ€ìš©ëŸ‰ ë°ì´í„° íˆìŠ¤í† ê·¸ë¨ ìµœì í™”
            )
            
            valid_sets = [train_data]
            valid_names = ['train']
            
            if X_val is not None and y_val is not None:
                val_data = lgb.Dataset(
                    X_val, 
                    label=y_val, 
                    reference=train_data, 
                    free_raw_data=False
                )
                valid_sets.append(val_data)
                valid_names.append('valid')
            
            callbacks = []
            early_stopping = self.params.get('early_stopping_rounds', 250)
            if early_stopping:
                callbacks.append(lgb.early_stopping(early_stopping, verbose=False))
                
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° í•™ìŠµ ë¡œê·¸ ìµœì í™”
            if len(X_train) > 1000000:
                callbacks.append(lgb.log_evaluation(period=500))
            
            self.model = lgb.train(
                self.params,
                train_data,
                valid_sets=valid_sets,
                valid_names=valid_names,
                callbacks=callbacks
            )
            
            self.is_fitted = True
            
            # í•™ìŠµ ì‹œ ìŠ¤ì¼€ì¼ë§ ì ìš©
            if hasattr(self, 'scaler'):
                X_processed_scaled = self.scaler.transform(X_processed)
            else:
                X_processed_scaled = X_processed
            
            proba = self.model.predict_proba(X_processed_scaled)
            if proba.ndim == 2:
                proba = proba[:, 1]
            
            proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            # 1070ë§Œí–‰ ë°ì´í„° ê¸°ì¤€ ë‹¤ì–‘ì„± ê²€ì¦
            unique_count = len(np.unique(proba))
            expected_diversity = max(1000, len(proba) // 10000)
            
            if unique_count < expected_diversity:
                logger.warning(f"Logistic: ì˜ˆì¸¡ê°’ ë‹¤ì–‘ì„± ë¶€ì¡± (ê³ ìœ ê°’: {unique_count}, ê¸°ëŒ€ê°’: {expected_diversity})")
                noise = np.random.normal(0, proba.std() * 0.005, len(proba))
                proba = proba + noise
                proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            return proba
        except Exception as e:
            logger.error(f"Logistic ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return np.full(len(X), self.target_ctr)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """calibrationì´ ì ìš©ëœ í™•ë¥  ì˜ˆì¸¡"""
        raw_pred = self.predict_proba_raw(X)
        
        if self.is_calibrated and self.calibrator is not None:
            try:
                calibrated_pred = self.calibrator.predict_proba(raw_pred.reshape(-1, 1))[:, 1]
                return np.clip(calibrated_pred, 1e-15, 1 - 1e-15)
            except:
                pass
        
        return raw_pred

class CTRCalibrator:
    """CTR íŠ¹í™” í™•ë¥  ë³´ì • í´ë˜ìŠ¤ - 1070ë§Œí–‰ ìµœì í™”"""
    
    def __init__(self, target_ctr: float = 0.0201):
        self.target_ctr = target_ctr
        self.platt_scaler = None
        self.isotonic_regressor = None
        self.bias_correction = 0.0
        self.temperature_scaler = None
        self.distribution_matcher = None
        
    def fit_platt_scaling(self, y_true: np.ndarray, y_pred: np.ndarray):
        """Platt Scaling í•™ìŠµ - ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”"""
        if not SKLEARN_AVAILABLE:
            logger.warning("scikit-learnì´ ì—†ì–´ Platt scalingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
            
        try:
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§
            if len(y_true) > 100000:
                sample_indices = np.random.choice(len(y_true), 100000, replace=False)
                y_true_sample = y_true[sample_indices]
                y_pred_sample = y_pred[sample_indices]
            else:
                y_true_sample = y_true
                y_pred_sample = y_pred
            
            self.platt_scaler = LogisticRegression(
                random_state=42,
                max_iter=2000,
                class_weight={0: 1, 1: 49.75}
            )
            self.platt_scaler.fit(y_pred_sample.reshape(-1, 1), y_true_sample)
            logger.info("Platt scaling í•™ìŠµ ì™„ë£Œ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”)")
        except Exception as e:
            logger.error(f"Platt scaling í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
    
    def fit_isotonic_regression(self, y_true: np.ndarray, y_pred: np.ndarray):
        """Isotonic Regression í•™ìŠµ - ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”"""
        if not SKLEARN_AVAILABLE:
            logger.warning("scikit-learnì´ ì—†ì–´ Isotonic regressionì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
            
        try:
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§
            if len(y_true) > 100000:
                sample_indices = np.random.choice(len(y_true), 100000, replace=False)
                y_true_sample = y_true[sample_indices]
                y_pred_sample = y_pred[sample_indices]
            else:
                y_true_sample = y_true
                y_pred_sample = y_pred
            
            self.isotonic_regressor = IsotonicRegression(
                out_of_bounds='clip',
                increasing=True
            )
            self.isotonic_regressor.fit(y_pred_sample, y_true_sample)
            logger.info("Isotonic regression í•™ìŠµ ì™„ë£Œ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”)")
        except Exception as e:
            logger.error(f"Isotonic regression í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
    
    def fit_temperature_scaling(self, y_true: np.ndarray, y_pred: np.ndarray):
        """Temperature Scaling í•™ìŠµ - CTR íŠ¹í™”"""
        try:
            from scipy.optimize import minimize_scalar
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§
            if len(y_true) > 50000:
                sample_indices = np.random.choice(len(y_true), 50000, replace=False)
                y_true_sample = y_true[sample_indices]
                y_pred_sample = y_pred[sample_indices]
            else:
                y_true_sample = y_true
                y_pred_sample = y_pred
            
            def temperature_loss(temperature):
                if temperature <= 0.1:
                    return float('inf')
                
                # Logit ë³€í™˜
                pred_clipped = np.clip(y_pred_sample, 1e-15, 1 - 1e-15)
                logits = np.log(pred_clipped / (1 - pred_clipped))
                
                # Temperature ì ìš©
                calibrated_logits = logits / temperature
                calibrated_probs = 1 / (1 + np.exp(-calibrated_logits))
                calibrated_probs = np.clip(calibrated_probs, 1e-15, 1 - 1e-15)
                
                # Log loss + CTR í¸í–¥ íŒ¨ë„í‹°
                log_loss = -np.mean(y_true_sample * np.log(calibrated_probs) + 
                                  (1 - y_true_sample) * np.log(1 - calibrated_probs))
                
                # CTR í¸í–¥ íŒ¨ë„í‹°
                predicted_ctr = calibrated_probs.mean()
                actual_ctr = y_true_sample.mean()
                ctr_bias_penalty = abs(predicted_ctr - actual_ctr) * 1000
                
                return log_loss + ctr_bias_penalty
            
            result = minimize_scalar(temperature_loss, bounds=(0.1, 10.0), method='bounded')
            self.temperature_scaler = result.x
            logger.info(f"Temperature scaling í•™ìŠµ ì™„ë£Œ: T={self.temperature_scaler:.3f}")
        except Exception as e:
            logger.error(f"Temperature scaling í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
    
    def fit_bias_correction(self, y_true: np.ndarray, y_pred: np.ndarray):
        """ë‹¨ìˆœ í¸í–¥ ë³´ì • í•™ìŠµ - CTR íŠ¹í™”"""
        try:
            predicted_ctr = y_pred.mean()
            actual_ctr = y_true.mean()
            self.bias_correction = actual_ctr - predicted_ctr
            
            # CTR ë¶„í¬ ë§¤ì¹­
            self._fit_distribution_matching(y_true, y_pred)
            
            logger.info(f"í¸í–¥ ë³´ì • í•™ìŠµ ì™„ë£Œ: {self.bias_correction:.4f}")
        except Exception as e:
            logger.error(f"í¸í–¥ ë³´ì • í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
    
    def _fit_distribution_matching(self, y_true: np.ndarray, y_pred: np.ndarray):
        """CTR ë¶„í¬ ë§¤ì¹­ í•™ìŠµ"""
        try:
            # ë¶„ìœ„ìˆ˜ë³„ ë§¤ì¹­
            quantiles = [0.5, 0.75, 0.9, 0.95, 0.99]
            pred_quantiles = np.percentile(y_pred, [q * 100 for q in quantiles])
            
            self.distribution_matcher = {}
            
            for i, q in enumerate(quantiles):
                threshold = pred_quantiles[i]
                high_pred_mask = y_pred >= threshold
                
                if high_pred_mask.sum() > 0:
                    actual_rate_in_quantile = y_true[high_pred_mask].mean()
                    pred_rate_in_quantile = y_pred[high_pred_mask].mean()
                    
                    if pred_rate_in_quantile > 0:
                        correction_factor = actual_rate_in_quantile / pred_rate_in_quantile
                        self.distribution_matcher[q] = {
                            'threshold': threshold,
                            'correction_factor': correction_factor
                        }
            
            logger.info("CTR ë¶„í¬ ë§¤ì¹­ í•™ìŠµ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë¶„í¬ ë§¤ì¹­ í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    def apply_calibration(self, y_pred: np.ndarray, method: str = 'combined') -> np.ndarray:
        """ë³´ì • ì ìš© - CTR ìµœì í™”"""
        try:
            calibrated = y_pred.copy()
            
            if method == 'platt' and self.platt_scaler is not None:
                calibrated = self.platt_scaler.predict_proba(calibrated.reshape(-1, 1))[:, 1]
            
            elif method == 'isotonic' and self.isotonic_regressor is not None:
                calibrated = self.isotonic_regressor.predict(calibrated)
            
            elif method == 'temperature' and self.temperature_scaler is not None:
                pred_clipped = np.clip(calibrated, 1e-15, 1 - 1e-15)
                logits = np.log(pred_clipped / (1 - pred_clipped))
                calibrated_logits = logits / self.temperature_scaler
                calibrated = 1 / (1 + np.exp(-calibrated_logits))
            
            elif method == 'combined':
                # ë‹¤ë‹¨ê³„ ë³´ì •
                if self.platt_scaler is not None:
                    calibrated = self.platt_scaler.predict_proba(calibrated.reshape(-1, 1))[:, 1]
                
                if self.temperature_scaler is not None:
                    pred_clipped = np.clip(calibrated, 1e-15, 1 - 1e-15)
                    logits = np.log(pred_clipped / (1 - pred_clipped))
                    calibrated_logits = logits / self.temperature_scaler
                    calibrated = 1 / (1 + np.exp(-calibrated_logits))
                
                # ë¶„í¬ ë§¤ì¹­ ì ìš©
                if self.distribution_matcher:
                    calibrated = self._apply_distribution_matching(calibrated)
            
            # í¸í–¥ ë³´ì •
            calibrated = calibrated + self.bias_correction
            
            # ìµœì¢… í´ë¦¬í•‘
            calibrated = np.clip(calibrated, 0.001, 0.999)
            
            return calibrated
            
        except Exception as e:
            logger.error(f"ë³´ì • ì ìš© ì‹¤íŒ¨: {e}")
            return np.clip(y_pred, 0.001, 0.999)
    
    def _apply_distribution_matching(self, y_pred: np.ndarray) -> np.ndarray:
        """ë¶„í¬ ë§¤ì¹­ ì ìš©"""
        try:
            calibrated = y_pred.copy()
            
            for quantile in sorted(self.distribution_matcher.keys(), reverse=True):
                matcher = self.distribution_matcher[quantile]
                threshold = matcher['threshold']
                correction_factor = matcher['correction_factor']
                
                high_pred_mask = calibrated >= threshold
                if high_pred_mask.sum() > 0:
                    calibrated[high_pred_mask] *= correction_factor
            
            return calibrated
        except Exception as e:
            logger.warning(f"ë¶„í¬ ë§¤ì¹­ ì ìš© ì‹¤íŒ¨: {e}")
            return y_pred

class ModelFactory:
    """CTR íŠ¹í™” ëª¨ë¸ íŒ©í† ë¦¬ í´ë˜ìŠ¤ - 1070ë§Œí–‰ ìµœì í™”"""
    
    @staticmethod
    def create_model(model_type: str, **kwargs) -> BaseModel:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ CTR íŠ¹í™” ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        
        try:
            if model_type.lower() == 'lightgbm':
                if not LIGHTGBM_AVAILABLE:
                    raise ImportError("LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return LightGBMModel(kwargs.get('params'))
            
            elif model_type.lower() == 'xgboost':
                if not XGBOOST_AVAILABLE:
                    raise ImportError("XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return XGBoostModel(kwargs.get('params'))
            
            elif model_type.lower() == 'catboost':
                if not CATBOOST_AVAILABLE:
                    raise ImportError("CatBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return CatBoostModel(kwargs.get('params'))
            
            elif model_type.lower() == 'deepctr':
                if not TORCH_AVAILABLE:
                    raise ImportError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                input_dim = kwargs.get('input_dim')
                if input_dim is None:
                    raise ValueError("DeepCTR ëª¨ë¸ì—ëŠ” input_dimì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return DeepCTRModel(input_dim, kwargs.get('params'))
            
            elif model_type.lower() == 'logistic':
                return LogisticModel(kwargs.get('params'))
            
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ ({model_type}): {str(e)}")
            raise
    
    @staticmethod
    def get_available_models() -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íƒ€ì… ë¦¬ìŠ¤íŠ¸ - 1070ë§Œí–‰ ìµœì í™” ì •ë³´ í¬í•¨"""
        available = []
        
        if SKLEARN_AVAILABLE:
            available.append('logistic')
        
        if LIGHTGBM_AVAILABLE:
            available.append('lightgbm')
        if XGBOOST_AVAILABLE:
            available.append('xgboost')
        if CATBOOST_AVAILABLE:
            available.append('catboost')
        if TORCH_AVAILABLE:
            available.append('deepctr')
        
        if not available:
            logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
            available = ['logistic']
        
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ (1070ë§Œí–‰ ìµœì í™”): {available}")
        
        return available

class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤ - 1070ë§Œí–‰ ìµœì í™”"""
    
    @staticmethod
    def evaluate_model(model: BaseModel, 
                      X_test: pd.DataFrame, 
                      y_test: pd.Series) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€ ìˆ˜í–‰ - Combined Score 0.30+ ëª©í‘œ"""
        
        try:
            logger.info(f"{model.name} ëª¨ë¸ í‰ê°€ ì‹œì‘ (í…ŒìŠ¤íŠ¸ í¬ê¸°: {len(X_test):,}í–‰)")
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì˜ˆì¸¡
            batch_size = 100000
            total_predictions = []
            
            for i in range(0, len(X_test), batch_size):
                end_idx = min(i + batch_size, len(X_test))
                X_batch = X_test.iloc[i:end_idx]
                
                try:
                    batch_pred = model.predict_proba(X_batch)
                    total_predictions.append(batch_pred)
                except Exception as e:
                    logger.warning(f"ë°°ì¹˜ {i//batch_size + 1} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    total_predictions.append(np.full(len(X_batch), 0.0201))
            
            y_pred_proba = np.concatenate(total_predictions)
            y_pred = (y_pred_proba >= 0.5).astype(int)
            
            metrics = {}
            
            if SKLEARN_AVAILABLE:
                try:
                    metrics['auc'] = roc_auc_score(y_test, y_pred_proba)
                    metrics['logloss'] = log_loss(y_test, y_pred_proba)
                except:
                    metrics['auc'] = 0.5
                    metrics['logloss'] = 1.0
                
                from sklearn.metrics import precision_score, recall_score, f1_score
                try:
                    metrics['precision'] = precision_score(y_test, y_pred, zero_division=0)
                    metrics['recall'] = recall_score(y_test, y_pred, zero_division=0)
                    metrics['f1'] = f1_score(y_test, y_pred, zero_division=0)
                except:
                    metrics['precision'] = 0.0
                    metrics['recall'] = 0.0
                    metrics['f1'] = 0.0
            else:
                metrics['auc'] = 0.5
                metrics['logloss'] = 1.0
                metrics['precision'] = 0.0
                metrics['recall'] = 0.0
                metrics['f1'] = 0.0
            
            metrics['accuracy'] = (y_test == y_pred).mean()
            
            # CTR ë¶„ì„ - 1070ë§Œí–‰ íŠ¹í™”
            metrics['ctr_actual'] = y_test.mean()
            metrics['ctr_predicted'] = y_pred_proba.mean()
            metrics['ctr_bias'] = metrics['ctr_predicted'] - metrics['ctr_actual']
            metrics['ctr_absolute_error'] = abs(metrics['ctr_bias'])
            
            # Combined Score ê³„ì‚°
            from evaluation import CTRMetrics
            ctr_metrics = CTRMetrics()
            metrics['combined_score'] = ctr_metrics.combined_score(y_test, y_pred_proba)
            
            # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
            metrics['target_achieved'] = metrics['combined_score'] >= 0.30
            
            logger.info(f"{model.name} í‰ê°€ ì™„ë£Œ - Combined Score: {metrics['combined_score']:.4f}")
            if metrics['target_achieved']:
                logger.info(f"ğŸ¯ {model.name} Combined Score 0.30+ ëª©í‘œ ë‹¬ì„±!")
            
        except Exception as e:
            logger.error(f"í‰ê°€ ì§€í‘œ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            metrics = {
                'auc': 0.5,
                'logloss': 1.0,
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'ctr_actual': 0.0201,
                'ctr_predicted': 0.0201,
                'ctr_bias': 0.0,
                'ctr_absolute_error': 0.0,
                'combined_score': 0.0,
                'target_achieved': False
            }
        
        return metricsê²°ê³¼ ë¡œê¹…
            if hasattr(self.model, 'best_iteration'):
                logger.info(f"{self.name} ìµœì  ë°˜ë³µ: {self.model.best_iteration}")
            
            feature_importance = self.model.feature_importance(importance_type='gain')
            top_features = np.argsort(feature_importance)[-10:][::-1]
            logger.info(f"ìƒìœ„ 10 ì¤‘ìš” í”¼ì²˜: {[self.feature_names[i] for i in top_features]}")
            
            logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
            del train_data
            if 'val_data' in locals():
                del val_data
            gc.collect()
            
        except Exception as e:
            logger.error(f"LightGBM í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            gc.collect()
            raise
        
        return self
    
    def predict_proba_raw(self, X: pd.DataFrame) -> np.ndarray:
        """ë³´ì •ë˜ì§€ ì•Šì€ ì›ë³¸ ì˜ˆì¸¡ - ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            X_processed = self._ensure_feature_consistency(X)
            X_processed = X_processed.fillna(0)
            
            # ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in X_processed.columns:
                if X_processed[col].dtype in ['float64']:
                    X_processed[col] = X_processed[col].astype('float32')
            
            num_iteration = getattr(self.model, 'best_iteration', None)
            proba = self.model.predict(X_processed, num_iteration=num_iteration)
            
            # ì˜ˆì¸¡ê°’ ìœ íš¨ì„± ê²€ì¦ ë° ë‹¤ì–‘ì„± ë³´ì¥
            proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            # 1070ë§Œí–‰ ë°ì´í„° ê¸°ì¤€ ë‹¤ì–‘ì„± ê²€ì¦
            unique_count = len(np.unique(proba))
            expected_diversity = max(1000, len(proba) // 10000)
            
            if unique_count < expected_diversity:
                logger.warning(f"LightGBM: ì˜ˆì¸¡ê°’ ë‹¤ì–‘ì„± ë¶€ì¡± (ê³ ìœ ê°’: {unique_count}, ê¸°ëŒ€ê°’: {expected_diversity})")
                noise = np.random.normal(0, proba.std() * 0.005, len(proba))
                proba = proba + noise
                proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            return proba
        except Exception as e:
            logger.error(f"LightGBM ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return np.full(len(X), self.target_ctr)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """calibrationì´ ì ìš©ëœ í™•ë¥  ì˜ˆì¸¡"""
        raw_pred = self.predict_proba_raw(X)
        
        if self.is_calibrated and self.calibrator is not None:
            try:
                calibrated_pred = self.calibrator.predict_proba(raw_pred.reshape(-1, 1))[:, 1]
                return np.clip(calibrated_pred, 1e-15, 1 - 1e-15)
            except:
                pass
        
        return raw_pred

class XGBoostModel(BaseModel):
    """CTR íŠ¹í™” XGBoost ëª¨ë¸ - GPU ìµœì í™”"""
    
    def __init__(self, params: Dict[str, Any] = None):
        if not XGBOOST_AVAILABLE:
            raise ImportError("XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        gpu_available = False
        if TORCH_AVAILABLE:
            try:
                import torch
                if torch.cuda.is_available():
                    test_tensor = torch.zeros(100, 100).cuda()
                    del test_tensor
                    torch.cuda.empty_cache()
                    gpu_available = True
            except:
                gpu_available = False
        
        # 1070ë§Œí–‰ + RTX 4060 Ti ìµœì í™” ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        default_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'tree_method': 'gpu_hist' if gpu_available else 'hist',
            'gpu_id': 0 if gpu_available else None,
            'max_depth': 12,
            'learning_rate': 0.05,
            'subsample': 0.75,
            'colsample_bytree': 0.75,
            'colsample_bylevel': 0.75,
            'colsample_bynode': 0.75,
            'min_child_weight': 25,
            'reg_alpha': 4.0,
            'reg_lambda': 4.0,
            'scale_pos_weight': 49.75,
            'random_state': Config.RANDOM_STATE,
            'n_estimators': 5000,
            'early_stopping_rounds': 250,
            'max_bin': 128 if gpu_available else 255,  # GPU ë©”ëª¨ë¦¬ ê³ ë ¤
            'nthread': 12,
            'grow_policy': 'lossguide',
            'max_leaves': 511,
            'gamma': 0.2,
            'max_delta_step': 0,
            'predictor': 'gpu_predictor' if gpu_available else 'cpu_predictor',
            'sampling_method': 'uniform',
            'reg_lambda_bias': 0.0
        }
        
        if gpu_available:
            default_params['gpu_ram_part'] = 0.6  # RTX 4060 Ti 16GBì˜ 60% ì‚¬ìš©
            default_params['single_precision_histogram'] = True
        else:
            default_params.pop('gpu_id', None)
            default_params.pop('single_precision_histogram', None)
        
        if params:
            default_params.update(params)
        
        default_params = self._validate_gpu_params(default_params, gpu_available)
        
        super().__init__("XGBoost", default_params)
    
    def _validate_gpu_params(self, params: Dict[str, Any], gpu_available: bool) -> Dict[str, Any]:
        """XGBoost GPU íŒŒë¼ë¯¸í„° ê²€ì¦ ê°•í™”"""
        safe_params = params.copy()
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì„¤ì •
        if 'objective' not in safe_params:
            safe_params['objective'] = 'binary:logistic'
        if 'eval_metric' not in safe_params:
            safe_params['eval_metric'] = 'logloss'
        
        # GPU íŠ¹í™” ì„¤ì •
        if gpu_available:
            safe_params['tree_method'] = 'gpu_hist'
            safe_params['gpu_id'] = 0
            safe_params['max_bin'] = min(safe_params.get('max_bin', 128), 128)  # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            safe_params['predictor'] = 'gpu_predictor'
            safe_params['single_precision_histogram'] = True
            safe_params['gpu_ram_part'] = min(safe_params.get('gpu_ram_part', 0.6), 0.7)
        else:
            safe_params['tree_method'] = 'hist'
            safe_params.pop('gpu_id', None)
            safe_params.pop('single_precision_histogram', None)
            safe_params.pop('gpu_ram_part', None)
            safe_params['max_bin'] = min(safe_params.get('max_bin', 255), 255)
            safe_params['predictor'] = 'cpu_predictor'
        
        # 1070ë§Œí–‰ íŠ¹í™” ë²”ìœ„ ì œí•œ
        safe_params['max_depth'] = min(max(safe_params.get('max_depth', 12), 3), 18)
        safe_params['nthread'] = min(safe_params.get('nthread', 12), 12)
        
        # CTR íŠ¹í™” ì •ê·œí™” ê°•í™”
        safe_params['reg_alpha'] = max(safe_params.get('reg_alpha', 4.0), 2.0)
        safe_params['reg_lambda'] = max(safe_params.get('reg_lambda', 4.0), 2.0)
        safe_params['min_child_weight'] = max(safe_params.get('min_child_weight', 25), 10)
        safe_params['gamma'] = max(safe_params.get('gamma', 0.2), 0.0)
        
        # í•™ìŠµë¥  ë° ì„œë¸Œìƒ˜í”Œë§ ë²”ìœ„ ì œí•œ
        safe_params['learning_rate'] = min(max(safe_params.get('learning_rate', 0.05), 0.01), 0.3)
        safe_params['subsample'] = min(max(safe_params.get('subsample', 0.75), 0.5), 1.0)
        safe_params['colsample_bytree'] = min(max(safe_params.get('colsample_bytree', 0.75), 0.5), 1.0)
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì„±ëŠ¥ ìµœì í™”
        safe_params['grow_policy'] = 'lossguide'
        safe_params['max_leaves'] = min(safe_params.get('max_leaves', 511), 1023)
        
        return safe_params
    
    def fit(self, X_train: pd.DataFrame, y_train: pd.Series, 
            X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """CTR íŠ¹í™” XGBoost ëª¨ë¸ í•™ìŠµ - GPU ìµœì í™”"""
        logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë°ì´í„° í¬ê¸°: {len(X_train):,}í–‰)")
        
        gpu_info = ""
        if self.params.get('tree_method') == 'gpu_hist':
            try:
                import torch
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                gpu_info = f" [GPU: {gpu_name}, {gpu_memory:.1f}GB]"
            except:
                gpu_info = " [GPU ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨]"
        
        logger.info(f"XGBoost ì„¤ì •: {self.params.get('tree_method', 'hist')}{gpu_info}")
        
        try:
            self.feature_names = list(X_train.columns)
            self.training_data_size = len(X_train)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            X_train = X_train.fillna(0)
            if X_val is not None:
                X_val = X_val.fillna(0)
            
            # GPU ë©”ëª¨ë¦¬ ê³ ë ¤ ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in X_train.columns:
                if X_train[col].dtype in ['float64']:
                    X_train[col] = X_train[col].astype('float32')
                if X_val is not None and X_val[col].dtype in ['float64']:
                    X_val[col] = X_val[col].astype('float32')
            
            dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=False)
            
            evals = [(dtrain, 'train')]
            if X_val is not None and y_val is not None:
                dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=False)
                evals.append((dval, 'valid'))
            
            early_stopping = self.params.get('early_stopping_rounds', 250)
            
            # GPU í•™ìŠµ ì‹œ verbose ì¡°ì •
            verbose_eval = 500 if len(X_train) > 1000000 else False
            
            self.model = xgb.train(
                self.params,
                dtrain,
                evals=evals,
                early_stopping_rounds=early_stopping,
                verbose_eval=verbose_eval
            )
            
            self.is_fitted = True
            
            # í•™ìŠµ ê²°ê³¼ ë¡œê¹…
            if hasattr(self.model, 'best_iteration'):
                logger.info(f"{self.name} ìµœì  ë°˜ë³µ: {self.model.best_iteration}")
            
            feature_importance = self.model.get_score(importance_type='gain')
            if feature_importance:
                sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
                logger.info(f"ìƒìœ„ 5 ì¤‘ìš” í”¼ì²˜: {[f[0] for f in sorted_features[:5]]}")
            
            logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
            del dtrain
            if 'dval' in locals():
                del dval
            gc.collect()
            
        except Exception as e:
            logger.error(f"XGBoost í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            if 'gpu' in str(e).lower() and self.params.get('tree_method') == 'gpu_hist':
                logger.info("GPU í•™ìŠµ ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„")
                self.params['tree_method'] = 'hist'
                self.params.pop('gpu_id', None)
                self.params.pop('single_precision_histogram', None)
                self.params.pop('gpu_ram_part', None)
                self.params['predictor'] = 'cpu_predictor'
                self.params['max_bin'] = 255
                return self.fit(X_train, y_train, X_val, y_val)
            
            gc.collect()
            raise
        
        return self
    
    def predict_proba_raw(self, X: pd.DataFrame) -> np.ndarray:
        """ë³´ì •ë˜ì§€ ì•Šì€ ì›ë³¸ ì˜ˆì¸¡ - GPU ìµœì í™”"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            X_processed = self._ensure_feature_consistency(X)
            X_processed = X_processed.fillna(0)
            
            # ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in X_processed.columns:
                if X_processed[col].dtype in ['float64']:
                    X_processed[col] = X_processed[col].astype('float32')
            
            dtest = xgb.DMatrix(X_processed, enable_categorical=False)
            
            if hasattr(self.model, 'best_iteration') and self.model.best_iteration is not None:
                proba = self.model.predict(dtest, iteration_range=(0, self.model.best_iteration + 1))
            else:
                proba = self.model.predict(dtest)
            
            # ì˜ˆì¸¡ê°’ ìœ íš¨ì„± ê²€ì¦ ë° ë‹¤ì–‘ì„± ë³´ì¥
            proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            # 1070ë§Œí–‰ ë°ì´í„° ê¸°ì¤€ ë‹¤ì–‘ì„± ê²€ì¦
            unique_count = len(np.unique(proba))
            expected_diversity = max(1000, len(proba) // 10000)
            
            if unique_count < expected_diversity:
                logger.warning(f"XGBoost: ì˜ˆì¸¡ê°’ ë‹¤ì–‘ì„± ë¶€ì¡± (ê³ ìœ ê°’: {unique_count}, ê¸°ëŒ€ê°’: {expected_diversity})")
                noise = np.random.normal(0, proba.std() * 0.005, len(proba))
                proba = proba + noise
                proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            del dtest
            
            return proba
        except Exception as e:
            logger.error(f"XGBoost ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return np.full(len(X), self.target_ctr)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """calibrationì´ ì ìš©ëœ í™•ë¥  ì˜ˆì¸¡"""
        raw_pred = self.predict_proba_raw(X)
        
        if self.is_calibrated and self.calibrator is not None:
            try:
                calibrated_pred = self.calibrator.predict_proba(raw_pred.reshape(-1, 1))[:, 1]
                return np.clip(calibrated_pred, 1e-15, 1 - 1e-15)
            except:
                pass
        
        return raw_pred

class CatBoostModel(BaseModel):
    """CTR íŠ¹í™” CatBoost ëª¨ë¸ - ì™„ì „ ì¶©ëŒ ë°©ì§€ + GPU ìµœì í™”"""
    
    def __init__(self, params: Dict[str, Any] = None):
        if not CATBOOST_AVAILABLE:
            raise ImportError("CatBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        gpu_available = False
        if TORCH_AVAILABLE:
            try:
                import torch
                if torch.cuda.is_available():
                    test_tensor = torch.zeros(100, 100).cuda()
                    del test_tensor
                    torch.cuda.empty_cache()
                    gpu_available = True
            except:
                gpu_available = False
        
        # 1070ë§Œí–‰ + RTX 4060 Ti ìµœì í™” ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        default_params = {
            'loss_function': 'Logloss',
            'eval_metric': 'Logloss',
            'task_type': 'GPU' if gpu_available else 'CPU',
            'devices': '0' if gpu_available else None,
            'depth': 10,
            'learning_rate': 0.05,
            'l2_leaf_reg': 20,
            'iterations': 5000,
            'random_seed': Config.RANDOM_STATE,
            'verbose': False,
            'auto_class_weights': 'Balanced',
            'max_ctr_complexity': 2,
            'thread_count': 12,
            'bootstrap_type': 'Bayesian',
            'bagging_temperature': 1.5,
            'leaf_estimation_iterations': 15,
            'leaf_estimation_method': 'Newton',
            'grow_policy': 'Lossguide',
            'max_leaves': 511,
            'min_data_in_leaf': 200,
            'od_wait': 250,
            'od_type': 'IncToDec',
            'score_function': 'Cosine',
            'feature_border_type': 'GreedyLogSum',
            'leaf_estimation_backtracking': 'AnyImprovement',
            'boosting_type': 'Plain'
        }
        
        if gpu_available:
            default_params['gpu_ram_part'] = 0.6  # RTX 4060 Ti 16GBì˜ 60% ì‚¬ìš©
            default_params['gpu_cat_features_storage'] = 'GpuRam'
        else:
            default_params.pop('devices', None)
            default_params.pop('gpu_ram_part', None)
            default_params.pop('gpu_cat_features_storage', None)
        
        if params:
            default_params.update(params)
        
        # íŒŒë¼ë¯¸í„° ì¶©ëŒ ì™„ì „ ë°©ì§€
        default_params = self._validate_conflict_free_params(default_params)
        
        super().__init__("CatBoost", default_params)
        
        # CatBoost ëª¨ë¸ ì´ˆê¸°í™” - ì¡°ê¸° ì¢…ë£Œ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì œì™¸
        init_params = {k: v for k, v in self.params.items() 
                      if k not in ['early_stopping_rounds', 'use_best_model', 'eval_set', 'od_wait', 'od_type']}
        
        try:
            self.model = CatBoostClassifier(**init_params)
        except Exception as e:
            logger.error(f"CatBoost ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if any(keyword in str(e).lower() for keyword in ['gpu', 'cuda', 'device']):
                logger.info("GPU ì´ˆê¸°í™” ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„")
                self.params['task_type'] = 'CPU'
                self.params.pop('devices', None)
                self.params.pop('gpu_ram_part', None)
                self.params.pop('gpu_cat_features_storage', None)
                init_params = {k: v for k, v in self.params.items() 
                              if k not in ['early_stopping_rounds', 'use_best_model', 'eval_set', 'od_wait', 'od_type']}
                try:
                    self.model = CatBoostClassifier(**init_params)
                except Exception as e2:
                    logger.error(f"CPU ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {e2}")
                    raise
            else:
                raise
    
    def _validate_conflict_free_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """CatBoost íŒŒë¼ë¯¸í„° ì¶©ëŒ ì™„ì „ ë°©ì§€ - ê°•í™”"""
        safe_params = params.copy()
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì„¤ì •
        if 'loss_function' not in safe_params:
            safe_params['loss_function'] = 'Logloss'
        if 'verbose' not in safe_params:
            safe_params['verbose'] = False
        
        # ì¶©ëŒ ê°€ëŠ¥í•œ ëª¨ë“  íŒŒë¼ë¯¸í„° ì™„ì „ ì œê±°
        conflicting_params = [
            'early_stopping_rounds', 'use_best_model', 'eval_set', 
            'early_stopping', 'early_stop', 'best_model_min_trees',
            'stopping_rounds', 'use_best_iteration'
        ]
        
        removed_params = []
        for param in conflicting_params:
            if param in safe_params:
                removed_params.append(param)
                if param == 'early_stopping_rounds':
                    early_stop_val = safe_params.pop(param)
                    # od_wait ì„¤ì •ì€ fitì—ì„œ ì²˜ë¦¬
                    safe_params['_early_stopping_value'] = early_stop_val
                else:
                    safe_params.pop(param)
        
        if removed_params:
            logger.info(f"CatBoost: ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ì œê±°ëœ íŒŒë¼ë¯¸í„°: {removed_params}")
        
        # 1070ë§Œí–‰ íŠ¹í™” ë²”ìœ„ ì œí•œ
        safe_params['depth'] = min(max(safe_params.get('depth', 10), 4), 12)
        safe_params['thread_count'] = min(safe_params.get('thread_count', 12), 12)
        safe_params['iterations'] = min(safe_params.get('iterations', 5000), 10000)
        
        # CTR íŠ¹í™” ì •ê·œí™” ê°•í™”
        safe_params['l2_leaf_reg'] = max(safe_params.get('l2_leaf_reg', 20), 5)
        safe_params['min_data_in_leaf'] = max(safe_params.get('min_data_in_leaf', 200), 50)
        
        # í•™ìŠµë¥  ë²”ìœ„ ì œí•œ
        safe_params['learning_rate'] = min(max(safe_params.get('learning_rate', 0.05), 0.01), 0.3)
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì„±ëŠ¥ ìµœì í™”
        safe_params['grow_policy'] = 'Lossguide'
        safe_params['max_leaves'] = min(safe_params.get('max_leaves', 511), 1023)
        
        return safe_params
    
    def fit(self, X_train: pd.DataFrame, y_train: pd.Series, 
            X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """CTR íŠ¹í™” CatBoost ëª¨ë¸ í•™ìŠµ - ì™„ì „ ì¶©ëŒ ë°©ì§€"""
        logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë°ì´í„° í¬ê¸°: {len(X_train):,}í–‰)")
        
        gpu_info = ""
        if self.params.get('task_type') == 'GPU':
            try:
                import torch
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                gpu_info = f" [GPU: {gpu_name}, {gpu_memory:.1f}GB]"
            except:
                gpu_info = " [GPU ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨]"
        
        logger.info(f"CatBoost ì„¤ì •: {self.params.get('task_type', 'CPU')}{gpu_info}")
        
        try:
            self.feature_names = list(X_train.columns)
            self.training_data_size = len(X_train)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            X_train = X_train.fillna(0)
            if X_val is not None:
                X_val = X_val.fillna(0)
            
            # GPU ë©”ëª¨ë¦¬ ê³ ë ¤ ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in X_train.columns:
                if X_train[col].dtype in ['float64']:
                    X_train[col] = X_train[col].astype('float32')
                if X_val is not None and X_val[col].dtype in ['float64']:
                    X_val[col] = X_val[col].astype('float32')
            
            # fit ë©”ì„œë“œì—ì„œ ì¡°ê¸° ì¢…ë£Œ ê´€ë ¨ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
            fit_params = {
                'X': X_train,
                'y': y_train,
                'verbose': False,
                'plot': False
            }
            
            # ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¡°ê¸° ì¢…ë£Œ ì ìš©
            if X_val is not None and y_val is not None:
                fit_params['eval_set'] = (X_val, y_val)
                fit_params['use_best_model'] = True
                
                # early_stopping ê°’ì´ ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ ì ìš©
                early_stopping_value = self.params.pop('_early_stopping_value', 250)
                
                # CatBoost ëª¨ë¸ ì¬ì´ˆê¸°í™” (od íŒŒë¼ë¯¸í„° í¬í•¨)
                od_params = {k: v for k, v in self.params.items() 
                           if k not in ['early_stopping_rounds', 'use_best_model', 'eval_set', '_early_stopping_value']}
                od_params['od_wait'] = early_stopping_value
                od_params['od_type'] = 'IncToDec'
                
                try:
                    self.model = CatBoostClassifier(**od_params)
                except Exception as e:
                    logger.warning(f"od íŒŒë¼ë¯¸í„° í¬í•¨ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # od íŒŒë¼ë¯¸í„° ì œì™¸í•˜ê³  ì¬ì‹œë„
                    od_params = {k: v for k, v in od_params.items() 
                               if k not in ['od_wait', 'od_type']}
                    self.model = CatBoostClassifier(**od_params)
            
            # ëª¨ë¸ í•™ìŠµ
            self.model.fit(**fit_params)
            
            self.is_fitted = True
            
            # í•™ìŠµ ê²°ê³¼ ë¡œê¹…
            if hasattr(self.model, 'get_best_iteration'):
                try:
                    best_iter = self.model.get_best_iteration()
                    if best_iter is not None:
                        logger.info(f"{self.name} ìµœì  ë°˜ë³µ: {best_iter}")
                except:
                    pass
            
            feature_importance = self.model.get_feature_importance()
            if len(feature_importance) > 0:
                top_indices = np.argsort(feature_importance)[-5:][::-1]
                logger.info(f"ìƒìœ„ 5 ì¤‘ìš” í”¼ì²˜: {[self.feature_names[i] for i in top_indices]}")
            
            logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"CatBoost í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            
            # GPU ê´€ë ¨ ì˜¤ë¥˜ ì²˜ë¦¬
            if any(keyword in str(e).lower() for keyword in ['gpu', 'cuda', 'device']) and self.params.get('task_type') == 'GPU':
                logger.info("GPU í•™ìŠµ ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„")
                self.params['task_type'] = 'CPU'
                self.params.pop('devices', None)
                self.params.pop('gpu_ram_part', None)
                self.params.pop('gpu_cat_features_storage', None)
                try:
                    cpu_params = {k: v for k, v in self.params.items() 
                                 if k not in ['early_stopping_rounds', 'use_best_model', 'eval_set', 'od_wait', 'od_type', '_early_stopping_value']}
                    self.model = CatBoostClassifier(**cpu_params)
                    return self.fit(X_train, y_train, X_val, y_val)
                except Exception as e2:
                    logger.error(f"CPU ì¬ì‹œë„ë„ ì‹¤íŒ¨: {e2}")
            
            # ìµœì¢… ì‹œë„: ìµœì†Œ íŒŒë¼ë¯¸í„°
            try:
                logger.info("ìµœì†Œ íŒŒë¼ë¯¸í„°ë¡œ CatBoost í•™ìŠµ ì‹œë„")
                minimal_params = {
                    'loss_function': 'Logloss',
                    'task_type': 'CPU',
                    'depth': 8,
                    'learning_rate': 0.1,
                    'iterations': 1000,
                    'verbose': False,
                    'random_seed': self.params.get('random_seed', 42),
                    'auto_class_weights': 'Balanced'
                }
                self.model = CatBoostClassifier(**minimal_params)
                self.model.fit(X_train, y_train, verbose=False)
                self.is_fitted = True
                logger.info("ìµœì†Œ íŒŒë¼ë¯¸í„° CatBoost í•™ìŠµ ì™„ë£Œ")
            except Exception as e4:
                logger.error(f"ìµœì†Œ íŒŒë¼ë¯¸í„° í•™ìŠµë„ ì‹¤íŒ¨: {str(e4)}")
                raise
        
        gc.collect()
        
        return self
    
    def predict_proba_raw(self, X: pd.DataFrame) -> np.ndarray:
        """ë³´ì •ë˜ì§€ ì•Šì€ ì›ë³¸ ì˜ˆì¸¡"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            X_processed = self._ensure_feature_consistency(X)
            X_processed = X_processed.fillna(0)
            
            # ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in X_processed.columns:
                if X_processed[col].dtype in ['float64']:
                    X_processed[col] = X_processed[col].astype('float32')
            
            proba = self.model.predict_proba(X_processed)
            if proba.ndim == 2:
                proba = proba[:, 1]
            
            # ì˜ˆì¸¡ê°’ ìœ íš¨ì„± ê²€ì¦ ë° ë‹¤ì–‘ì„± ë³´ì¥
            proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            # 1070ë§Œí–‰ ë°ì´í„° ê¸°ì¤€ ë‹¤ì–‘ì„± ê²€ì¦
            unique_count = len(np.unique(proba))
            expected_diversity = max(1000, len(proba) // 10000)
            
            if unique_count < expected_diversity:
                logger.warning(f"CatBoost: ì˜ˆì¸¡ê°’ ë‹¤ì–‘ì„± ë¶€ì¡± (ê³ ìœ ê°’: {unique_count}, ê¸°ëŒ€ê°’: {expected_diversity})")
                noise = np.random.normal(0, proba.std() * 0.005, len(proba))
                proba = proba + noise
                proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            return proba
        except Exception as e:
            logger.error(f"CatBoost ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return np.full(len(X), self.target_ctr)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """calibrationì´ ì ìš©ëœ í™•ë¥  ì˜ˆì¸¡"""
        raw_pred = self.predict_proba_raw(X)
        
        if self.is_calibrated and self.calibrator is not None:
            try:
                calibrated_pred = self.calibrator.predict_proba(raw_pred.reshape(-1, 1))[:, 1]
                return np.clip(calibrated_pred, 1e-15, 1 - 1e-15)
            except:
                pass
        
        return raw_pred

class DeepCTRModel(BaseModel):
    """CTR íŠ¹í™” ë”¥ëŸ¬ë‹ ëª¨ë¸ - RTX 4060 Ti 16GB ìµœì í™”"""
    
    def __init__(self, input_dim: int, params: Dict[str, Any] = None):
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        BaseModel.__init__(self, "DeepCTR", params)
        
        # RTX 4060 Ti 16GB íŠ¹í™” ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        default_params = {
            'hidden_dims': [1024, 512, 256, 128, 64],
            'dropout_rate': 0.4,
            'learning_rate': 0.0008,
            'weight_decay': 1e-4,
            'batch_size': 2048,
            'epochs': 100,
            'patience': 20,
            'use_batch_norm': True,
            'activation': 'relu',
            'use_residual': True,
            'use_attention': False,
            'focal_loss_alpha': 0.25,
            'focal_loss_gamma': 2.0,
            'gradient_clip_norm': 1.0,
            'warmup_epochs': 5,
            'lr_scheduler': 'cosine',
            'label_smoothing': 0.01
        }
        
        if params:
            default_params.update(params)
        self.params = default_params
        
        self.input_dim = input_dim
        
        self.device = 'cpu'
        self.gpu_available = False
        
        if TORCH_AVAILABLE:
            try:
                if torch.cuda.is_available():
                    test_tensor = torch.zeros(2000, 2000).cuda()
                    test_result = test_tensor.sum().item()
                    del test_tensor
                    torch.cuda.empty_cache()
                    
                    # RTX 4060 Ti 16GB ìµœì í™” - 70% ì‚¬ìš©
                    torch.cuda.set_per_process_memory_fraction(0.7)
                    
                    self.device = 'cuda:0'
                    self.gpu_available = True
                    logger.info(f"GPU ë””ë°”ì´ìŠ¤ ì‚¬ìš© ì„¤ì • ì™„ë£Œ - RTX 4060 Ti 16GB ìµœì í™”")
                else:
                    logger.info("CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥, CPU ëª¨ë“œ")
            except Exception as e:
                logger.warning(f"GPU ì„¤ì • ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
                self.device = 'cpu'
                self.gpu_available = False
        
        try:
            self.network = self._build_ctr_network()
            self.optimizer = None
            
            self.scaler = None
            if AMP_AVAILABLE and self.gpu_available:
                try:
                    self.scaler = GradScaler()
                    logger.info("Mixed Precision í™œì„±í™” - RTX 4060 Ti ìµœì í™”")
                except:
                    self.scaler = None
                    logger.info("Mixed Precision ë¹„í™œì„±í™”")
            
            if TORCH_AVAILABLE:
                pos_weight = torch.tensor([49.75], device=self.device)  # ì‹¤ì œ CTR 0.0201 ë°˜ì˜
                self.criterion = self._get_ctr_loss(pos_weight)
                
                self.temperature = nn.Parameter(torch.ones(1, device=self.device) * 1.5)
                
                self.to(self.device)
                
        except Exception as e:
            logger.error(f"DeepCTR ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if self.gpu_available:
                logger.info("CPU ëª¨ë“œë¡œ ì¬ì‹œë„")
                self.device = 'cpu'
                self.gpu_available = False
                self.network = self._build_ctr_network()
                pos_weight = torch.tensor([49.75])
                self.criterion = self._get_ctr_loss(pos_weight)
                self.temperature = nn.Parameter(torch.ones(1) * 1.5)
                self.to(self.device)
            else:
                raise
    
    def _build_ctr_network(self):
        """CTR íŠ¹í™” ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„± - RTX 4060 Ti ìµœì í™”"""
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
        try:
            hidden_dims = self.params['hidden_dims']
            dropout_rate = self.params['dropout_rate']
            use_batch_norm = self.params.get('use_batch_norm', True)
            activation = self.params.get('activation', 'relu')
            use_residual = self.params.get('use_residual', True)
            
            layers = []
            prev_dim = self.input_dim
            
            # ì…ë ¥ ì •ê·œí™”
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(prev_dim))
            
            # íˆë“  ë ˆì´ì–´ êµ¬ì„±
            for i, hidden_dim in enumerate(hidden_dims):
                # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ ì°¨ì› í™•ì¸
                use_residual_this_layer = use_residual and prev_dim == hidden_dim
                
                linear = nn.Linear(prev_dim, hidden_dim)
                
                # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ê°œì„  - CTR íŠ¹í™”
                if activation == 'relu':
                    nn.init.kaiming_uniform_(linear.weight, mode='fan_in', nonlinearity='relu')
                elif activation in ['gelu', 'swish']:
                    nn.init.xavier_uniform_(linear.weight, gain=1.0)
                else:
                    nn.init.xavier_uniform_(linear.weight)
                
                nn.init.zeros_(linear.bias)
                
                layers.append(linear)
                
                if use_batch_norm:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                
                # í™œì„±í™” í•¨ìˆ˜
                if activation == 'relu':
                    layers.append(nn.ReLU(inplace=True))
                elif activation == 'gelu':
                    layers.append(nn.GELU())
                elif activation == 'swish':
                    layers.append(nn.SiLU())
                
                # ë“œë¡­ì•„ì›ƒ (ë§ˆì§€ë§‰ ë ˆì´ì–´ ì œì™¸)
                if i < len(hidden_dims) - 1:
                    layers.append(nn.Dropout(dropout_rate))
                
                prev_dim = hidden_dim
            
            # ì¶œë ¥ ë ˆì´ì–´
            output_layer = nn.Linear(prev_dim, 1)
            # CTR íŠ¹í™” ì¶œë ¥ ë ˆì´ì–´ ì´ˆê¸°í™”
            nn.init.xavier_uniform_(output_layer.weight, gain=0.1)
            nn.init.constant_(output_layer.bias, -3.0)  # CTR 0.02 ì´ˆê¸° í¸í–¥
            layers.append(output_layer)
            
            return nn.Sequential(*layers)
        except Exception as e:
            logger.error(f"ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _get_ctr_loss(self, pos_weight):
        """CTR íŠ¹í™” ì†ì‹¤í•¨ìˆ˜"""
        if self.params.get('use_focal_loss', False):
            return self._focal_loss
        else:
            if self.params.get('label_smoothing', 0) > 0:
                return self._label_smoothing_bce_loss
            else:
                return nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    
    def _focal_loss(self, inputs, targets):
        """Focal Loss êµ¬í˜„ - CTR íŠ¹í™”"""
        alpha = self.params['focal_loss_alpha']
        gamma = self.params['focal_loss_gamma']
        
        ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = alpha * (1-pt)**gamma * ce_loss
        
        return focal_loss.mean()
    
    def _label_smoothing_bce_loss(self, inputs, targets):
        """Label Smoothing BCE Loss"""
        smoothing = self.params.get('label_smoothing', 0.01)
        targets_smooth = targets * (1 - smoothing) + 0.5 * smoothing
        
        return nn.functional.binary_cross_entropy_with_logits(inputs, targets_smooth)
    
    def to(self, device):
        """ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™"""
        if TORCH_AVAILABLE:
            try:
                self.network = self.network.to(device)
                if hasattr(self, 'temperature'):
                    self.temperature = self.temperature.to(device)
                if hasattr(self, 'criterion') and hasattr(self.criterion, 'to'):
                    self.criterion = self.criterion.to(device)
                self.device = device
            except Exception as e:
                logger.warning(f"ë””ë°”ì´ìŠ¤ ì´ë™ ì‹¤íŒ¨: {e}")
                self.device = 'cpu'
                self.gpu_available = False
    
    def train(self, mode=True):
        """í•™ìŠµ ëª¨ë“œ ì„¤ì •"""
        if TORCH_AVAILABLE and hasattr(self, 'network'):
            self.network.train(mode)
    
    def eval(self):
        """í‰ê°€ ëª¨ë“œ ì„¤ì •"""
        if TORCH_AVAILABLE and hasattr(self, 'network'):
            self.network.eval()
    
    def parameters(self):
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        if TORCH_AVAILABLE and hasattr(self, 'network'):
            params = list(self.network.parameters())
            if hasattr(self, 'temperature'):
                params.append(self.temperature)
            return params
        return []
    
    def forward(self, x):
        """ìˆœì „íŒŒ"""
        if TORCH_AVAILABLE and hasattr(self, 'network'):
            return self.network(x).squeeze(-1)
        else:
            raise RuntimeError("PyTorchê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    def fit(self, X_train: pd.DataFrame, y_train: pd.Series, 
            X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """CTR íŠ¹í™” ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ - RTX 4060 Ti ìµœì í™”"""
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
        logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Device: {self.device}, ë°ì´í„° í¬ê¸°: {len(X_train):,}í–‰)")
        
        try:
            self.feature_names = list(X_train.columns)
            self.training_data_size = len(X_train)
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            X_train = X_train.fillna(0)
            if X_val is not None:
                X_val = X_val.fillna(0)
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™” ë°ì´í„° ì •ê·œí™”
            X_train_values = X_train.values.astype('float32')
            if X_val is not None:
                X_val_values = X_val.values.astype('float32')
            
            # í‘œì¤€í™” - CTR ëª¨ë¸ì— ì¤‘ìš”
            mean = X_train_values.mean(axis=0, keepdims=True)
            std = X_train_values.std(axis=0, keepdims=True) + 1e-8
            X_train_values = (X_train_values - mean) / std
            if X_val is not None:
                X_val_values = (X_val_values - mean) / std
            
            # Optimizer ì„¤ì • - RTX 4060 Ti ìµœì í™”
            self.optimizer = optim.AdamW(
                self.parameters(), 
                lr=self.params['learning_rate'],
                weight_decay=self.params.get('weight_decay', 1e-4),
                eps=1e-8,
                betas=(0.9, 0.999)
            )
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            if self.params.get('lr_scheduler') == 'cosine':
                scheduler = optim.lr_scheduler.CosineAnnealingLR(
                    self.optimizer, 
                    T_max=self.params['epochs'],
                    eta_min=self.params['learning_rate'] * 0.01
                )
            else:
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                    self.optimizer, 
                    mode='min', 
                    factor=0.8,
                    patience=15,
                    min_lr=1e-6
                )
            
            # RTX 4060 Ti 16GB ë©”ëª¨ë¦¬ ê³ ë ¤ ë°°ì¹˜ í¬ê¸°
            batch_size = min(self.params['batch_size'], 4096) if self.gpu_available else 1024
            
            X_train_tensor = torch.FloatTensor(X_train_values).to(self.device)
            y_train_tensor = torch.FloatTensor(y_train.values).to(self.device)
            
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = TorchDataLoader(
                train_dataset, 
                batch_size=batch_size, 
                shuffle=True,
                num_workers=0,
                pin_memory=False if self.gpu_available else False
            )
            
            val_loader = None
            if X_val is not None and y_val is not None:
                X_val_tensor = torch.FloatTensor(X_val_values).to(self.device)
                y_val_tensor = torch.FloatTensor(y_val.values).to(self.device)
                val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
                val_loader = TorchDataLoader(
                    val_dataset, 
                    batch_size=batch_size,
                    num_workers=0,
                    pin_memory=False
                )
            
            best_val_loss = float('inf')
            patience_counter = 0
            max_epochs = min(self.params['epochs'], 100)
            warmup_epochs = self.params.get('warmup_epochs', 5)
            
            for epoch in range(max_epochs):
                self.train()
                train_loss = 0.0
                batch_count = 0
                
                # Warmup í•™ìŠµë¥  ì¡°ì •
                if epoch < warmup_epochs:
                    warmup_lr = self.params['learning_rate'] * (epoch + 1) / warmup_epochs
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = warmup_lr
                
                for batch_X, batch_y in train_loader:
                    self.optimizer.zero_grad()
                    
                    try:
                        if self.scaler is not None and AMP_AVAILABLE:
                            with autocast():
                                logits = self.forward(batch_X)
                                loss = self.criterion(logits, batch_y)
                            
                            self.scaler.scale(loss).backward()
                            
                            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                            self.scaler.unscale_(self.optimizer)
                            torch.nn.utils.clip_grad_norm_(self.parameters(), 
                                                         max_norm=self.params.get('gradient_clip_norm', 1.0))
                            
                            self.scaler.step(self.optimizer)
                            self.scaler.update()
                        else:
                            logits = self.forward(batch_X)
                            loss = self.criterion(logits, batch_y)
                            
                            loss.backward()
                            
                            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                            torch.nn.utils.clip_grad_norm_(self.parameters(), 
                                                         max_norm=self.params.get('gradient_clip_norm', 1.0))
                            
                            self.optimizer.step()
                        
                        train_loss += loss.item()
                        batch_count += 1
                        
                        # RTX 4060 Ti ë©”ëª¨ë¦¬ ê´€ë¦¬
                        if batch_count % 20 == 0 and self.gpu_available:
                            torch.cuda.empty_cache()
                            
                    except Exception as e:
                        logger.warning(f"ë°°ì¹˜ í•™ìŠµ ì‹¤íŒ¨: {e}")
                        continue
                
                if batch_count == 0:
                    logger.error("ëª¨ë“  ë°°ì¹˜ í•™ìŠµì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                    break
                    
                train_loss /= batch_count
                
                # ê²€ì¦
                val_loss = train_loss
                if val_loader is not None:
                    self.eval()
                    val_loss = 0.0
                    val_batch_count = 0
                    
                    with torch.no_grad():
                        for batch_X, batch_y in val_loader:
                            try:
                                if self.scaler is not None and AMP_AVAILABLE:
                                    with autocast():
                                        logits = self.forward(batch_X)
                                        loss = self.criterion(logits, batch_y)
                                else:
                                    logits = self.forward(batch_X)
                                    loss = self.criterion(logits, batch_y)
                                
                                val_loss += loss.item()
                                val_batch_count += 1
                            except Exception as e:
                                logger.warning(f"ê²€ì¦ ë°°ì¹˜ ì‹¤íŒ¨: {e}")
                                continue
                    
                    if val_batch_count > 0:
                        val_loss /= val_batch_count
                        
                        if self.params.get('lr_scheduler') != 'cosine':
                            scheduler.step(val_loss)
                        
                        if val_loss < best_val_loss:
                            best_val_loss = val_loss
                            patience_counter = 0
                        else:
                            patience_counter += 1
                        
                        if patience_counter >= self.params['patience']:
                            logger.info(f"ì¡°ê¸° ì¢…ë£Œ: epoch {epoch + 1}")
                            break
                
                # Cosine ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
                if self.params.get('lr_scheduler') == 'cosine' and epoch >= warmup_epochs:
                    scheduler.step()
                
                if (epoch + 1) % 10 == 0:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    logger.info(f"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, LR = {current_lr:.6f}")
                
                if self.gpu_available:
                    torch.cuda.empty_cache()
            
            self.is_fitted = True
            logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"DeepCTR í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            if self.gpu_available and ('cuda' in str(e).lower() or 'gpu' in str(e).lower()):
                logger.info("GPU í•™ìŠµ ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„")
                self.device = 'cpu'
                self.gpu_available = False
                self.to('cpu')
                return self.fit(X_train, y_train, X_val, y_val)
            raise
        finally:
            if self.gpu_available:
                torch.cuda.empty_cache()
            gc.collect()
        
        return self
    
    def predict_proba_raw(self, X: pd.DataFrame) -> np.ndarray:
        """ë³´ì •ë˜ì§€ ì•Šì€ ì›ë³¸ ì˜ˆì¸¡ - RTX 4060 Ti ìµœì í™”"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            X_processed = self._ensure_feature_consistency(X)
            X_processed = X_processed.fillna(0)
            
            # í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì •ê·œí™” ì ìš©
            X_values = X_processed.values.astype('float32')
            
            # ê°„ë‹¨í•œ ì •ê·œí™”
            X_values = (X_values - X_values.mean(axis=0, keepdims=True)) / (X_values.std(axis=0, keepdims=True) + 1e-8)
            
            self.eval()
            X_tensor = torch.FloatTensor(X_values).to(self.device)
            
            predictions = []
            batch_size = min(self.params['batch_size'], 2048)
            
            with torch.no_grad():
                for i in range(0, len(X_tensor), batch_size):
                    batch = X_tensor[i:i + batch_size]
                    
                    try:
                        if self.scaler is not None and AMP_AVAILABLE:
                            with autocast():
                                logits = self.forward(batch)
                                proba = torch.sigmoid(logits / self.temperature)
                        else:
                            logits = self.forward(batch)
                            proba = torch.sigmoid(logits / self.temperature)
                        
                        predictions.append(proba.cpu().numpy())
                        
                        # RTX 4060 Ti ë©”ëª¨ë¦¬ ê´€ë¦¬
                        if self.gpu_available and i % (batch_size * 10) == 0:
                            torch.cuda.empty_cache()
                            
                    except Exception as e:
                        logger.warning(f"ì˜ˆì¸¡ ë°°ì¹˜ ì‹¤íŒ¨: {e}")
                        batch_size_actual = len(batch)
                        predictions.append(np.full(batch_size_actual, self.target_ctr))
            
            proba = np.concatenate(predictions)
            proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            # 1070ë§Œí–‰ ë°ì´í„° ê¸°ì¤€ ë‹¤ì–‘ì„± ê²€ì¦
            unique_count = len(np.unique(proba))
            expected_diversity = max(1000, len(proba) // 10000)
            
            if unique_count < expected_diversity:
                logger.warning(f"DeepCTR: ì˜ˆì¸¡ê°’ ë‹¤ì–‘ì„± ë¶€ì¡± (ê³ ìœ ê°’: {unique_count}, ê¸°ëŒ€ê°’: {expected_diversity})")
                noise = np.random.normal(0, proba.std() * 0.005, len(proba))
                proba = proba + noise
                proba = np.clip(proba, 1e-15, 1 - 1e-15)
            
            return proba
        except Exception as e:
            logger.error(f"DeepCTR ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return np.full(len(X), self.target_ctr)
        finally:
            if self.gpu_available:
                torch.cuda.empty_cache()
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """calibrationì´ ì ìš©ëœ í™•ë¥  ì˜ˆì¸¡"""
        raw_pred = self.predict_proba_raw(X)
        
        if self.is_calibrated and self.calibrator is not None:
            try:
                calibrated_pred = self.calibrator.predict_proba(raw_pred.reshape(-1, 1))[:, 1]
                return np.clip(calibrated_pred, 1e-15, 1 - 1e-15)
            except:
                pass
        
        return raw_pred

class LogisticModel(BaseModel):
    """ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ - ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”"""
    
    def __init__(self, params: Dict[str, Any] = None):
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # 1070ë§Œí–‰ ìµœì í™” ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        default_params = {
            'C': 0.01,  # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ë” ê°•í•œ ì •ê·œí™”
            'max_iter': 5000,
            'random_state': Config.RANDOM_STATE,
            'class_weight': {0: 1, 1: 49.75},  # ì‹¤ì œ CTR 0.0201 ë°˜ì˜
            'solver': 'lbfgs',
            'penalty': 'l2',
            'tol': 1e-6,
            'n_jobs': 12  # Ryzen 5 5600X 12ìŠ¤ë ˆë“œ
        }
        if params:
            default_params.update(params)
        super().__init__("LogisticRegression", default_params)
        
        self.model = LogisticRegression(**self.params)
    
    def fit(self, X_train: pd.DataFrame, y_train: pd.Series, 
            X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ - ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”"""
        logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ë°ì´í„° í¬ê¸°: {len(X_train):,}í–‰)")
        
        try:
            self.feature_names = list(X_train.columns)
            self.training_data_size = len(X_train)
            
            X_train = X_train.fillna(0)
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in X_train.columns:
                if X_train[col].dtype in ['float64']:
                    X_train[col] = X_train[col].astype('float32')
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ìŠ¤ì¼€ì¼ë§ ì ìš©
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            
            self.scaler = scaler  # ì˜ˆì¸¡ ì‹œ ì‚¬ìš©
            
            self.model.fit(X_train_scaled, y_train)
            self.is_fitted = True
            
            # ê³„ìˆ˜ ë¶„ì„
            if hasattr(self.model, 'coef_'):
                coef_importance = np.abs(self.model.coef_[0])
                top_indices = np.argsort(coef_importance)[-5:][::-1]
                logger.info(f"ìƒìœ„ 5 ì¤‘ìš” í”¼ì²˜: {[self.feature_names[i] for i in top_indices]}")
            
            logger.info(f"{self.name} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Logistic í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            raise
        
        return self
    
    def predict_proba_raw(self, X: pd.DataFrame) -> np.ndarray:
        """ë³´ì •ë˜ì§€ ì•Šì€ ì›ë³¸ ì˜ˆì¸¡"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            X_processed = self._ensure_feature_consistency(X)
            X_processed = X_processed.fillna(0)
            
            # ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in X_processed.columns:
                if X_processed[col].dtype in ['float64']:
                    X_processed[col] = X_processed[col].astype('float32')
            
            # í•™ìŠµ